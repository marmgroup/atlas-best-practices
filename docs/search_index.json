[
["index.html", "Pre-processing high frequency animal tracking data Outline", " Pre-processing high frequency animal tracking data Pratik R. Gupte and others 2020-08-04 Outline An outline here. "],
["background.html", "Section 1 Background 1.1 Install atlastools", " Section 1 Background Tracking individual movement has become a firmly established component of animal ecology, helped by the adoption of the movement ecology paradigm (Nathan et al. 2008; Holyoak et al. 2008) and the rapid proliferation of animal tracking technologies (Kays et al. 2015; Hussey et al. 2015). The animal tracking revolution has offered unprecedented insights into the causes and consequences of animal space-use, from the mechanisms behind collective behaviour (e.g. Strandburg-Peshkin et al. 2015), to how animal migrations engineer landscape level processes (Geremia et al. 2019). 1.1 Install atlastools # use either devtools or remotes to install # install.packages(&quot;devtools&quot;) # install using devtools devtools::install_github(&quot;pratikunterwegs/atlastools&quot;) "],
["cleaning-point-data.html", "Section 2 Cleaning point data 2.1 Filtering within spatio-temporal bounds 2.2 Filtering on covariates", " Section 2 Cleaning point data 2.1 Filtering within spatio-temporal bounds Here, we simulate some movement data using the smoove R package from [@gurarie2017]. library(data.table) library(ggplot2) library(atlastools) # uses smoove from Gurarie et al. (2017) library(smoove) nu &lt;- 2 tau &lt;- 5 dt &lt;- .1 ucvm1 &lt;- simulateUCVM(nu=nu, tau=tau, T.max = 100, dt = dt) # add a reflection to x_max + x_max / 10, y_max + y_max / 10 data &lt;- as.data.table(ucvm1$XY) data[, time := seq_len(nrow(data))] data &lt;- data[seq_len(1000), ] fwrite(data, &quot;data/sim_data_sec_02_01.csv&quot;) Here we read the simulated data in, and plot it to observe its extent. # read in the data data &lt;- data.table::fread(&quot;data/sim_data_sec_02_01.csv&quot;) # process the data to remove points in this range data_excluded &lt;- atlastools::atl_filter_bounds(data, x = &quot;x&quot;, y = &quot;y&quot;, x_range = c(-15, 0), y_range = c(-35, -15)) # process to keep points inside data_included &lt;- atlastools::atl_filter_bounds(data, x = &quot;x&quot;, y = &quot;y&quot;, x_range = c(-15, 0), y_range = c(-35, -15), remove_inside = FALSE) # join the data data[, type := &quot;raw&quot;] data_excluded[, type := &quot;bbox excluded&quot;] data_included[, type := &quot;bbox included&quot;] data &lt;- rbind(data, data_excluded, data_included) # arrange type data$type &lt;- factor(data$type, levels = c(&quot;raw&quot;, &quot;bbox excluded&quot;, &quot;bbox included&quot;)) Filtering by bounding box. Explicit spatial filtering is a finer method which examines whether the tracking data lie within a polygon, which may be irregular. This typically involves converting the data’s position coordinates to a multi-point spatial geometry, and finding the intersection with a polygon or multi-polygon geometry representing a study area. Combined with knowledge of species’ biology and the study site, explicit spatial filtering can be used to remove obviously erroneous positions such as those that would place an individual in an area it is physiologically incapable of traversing. The main drawback to explicit spatial filtering is that it is computationally intensive, and it is advisable to apply a coarse bounding box filter first in order to reduce the computational load, and only then filter by an irregular spatial feature. Both bounding box and explicit spatial filters can be combined with the timestamp of a position to remove locations where time is an important determinant of accessibility. For example, terrestrial species may be able to access the bottom of a waterbody only during certain times of day or year, and data whose time and position coordinates do not meet these criteria can be removed. Temporal filtering can also be applied independently of spatial filtering to exclude positions from certain time periods during tracking. These are typically periods when data are expected to be biased, such as where an animal has been fitted with a tracker but has not been released, or from the first day after fitting a tracker to avoid including movement behaviour biased by the stress of the capture. When combining spatial and temporal filters, it is better to apply the temporal filter first, since it is less computationally intensive than even a bounding box filter, and much less so than an explicit spatial filter, while reducing the number of positions that must be examined by these subsequent filters. In any case, broad filters must always be applied with caution; they are especially susceptible to excluding data that represent novel behaviour of which researchers are unaware, such as a new movement mode or changed local conditions which make it possible for species to move across previously inaccessible regions. 2.2 Filtering on covariates Simple spatial and temporal filters are agnostic to any correlation between the status of a tracking system and the extent of error. However, both may be refined by consulting tracking system metadata, i.e., data on the status of the tracking system itself, which can be a good indicator of the reliability of calculated positions. For example, at a broad scale the number of active receivers in an ATLAS system can indicate time periods when a system suffers a malfunction and thus collects unreliable data; these ‘down times’ can form the basis of a temporal filter. Similarly, data from system tests may reveal regions of the study area in which the system’s localisations are error-prone, and can inform an explicit spatial filter. At smaller scales the observation-specific metadata may be used to filter out locations which fail to meet a certain cutoff. For example, ATLAS systems calculate animal positions using the times of arrival of a radio transmission from each tag to multiple receiver stations. The number of receiver stations involved in each localisation is considered to be an indicator of data quality, and positions localised using fewer than three receivers (the minimum required for an ATLAS localisation) can be excluded. Further, tracking systems often yield internal measures of positioning error which are based on the dilution of precision. ATLAS and other time-of-arrival systems calculate positioning error in two dimensions (MacCurdy et al. 2009), while other systems may yield a three dimensional positioning error. Positioning error is an intuitive criterion by which to filter data, and this takes the form of locations with an estimated error above a certain threshold being excluded. There are two other ways of dealing with positioning errors, and of these the aggregation of positions over a time interval is discussed below. The more demanding method is to model the animal’s movement, for example, in a continuous time movement model (Calabrese et al. 2016; Noonan et al. 2019). Movement model methods can use the positioning error from the tracking system, in conjunction with calibration data, to estimate the real individual position and movement distances between positions. However, movement model methods are computationally intensive, make several assumptions about animal movement, and can be challenging to implement for the beginning practitioner, and are not discussed further. "],
["cleaning-movement-tracks.html", "Section 3 Cleaning movement tracks 3.1 Filtering on speed and turning angle 3.2 Challenges to speed filters", " Section 3 Cleaning movement tracks 3.1 Filtering on speed and turning angle Since the phenomenon represented by data from animal tracking studies is movement, it is appropriate and necessary to examine whether individual trajectories obtained by joining positions remaining from previous filtering steps are realistic. This is often a subjective assessment based on expert judgement of the observed movement tracks, which presents two main problems: first, subjective assessments are difficult to reproduce, which hampers consistent analyses across tracking periods. The second issue is that manual identification and correction of problematic trajectories is prohibitively time consuming when dealing with high volumes of data. The solution to both problems is to employ a hybrid approach, in which experts manually examine a subset of movement tracks (rather than positions) and attempt to identify common problems with the trajectories. Experts can then conceptualise an algorithm by which problematic sections of a trajectory may be removed, and automate the application of this algorithm to the wider dataset, under the assumption that the subset used to define the algorithm was representative of the full data. Algorithms for filtering trajectories are different from those for filtering point locations in that they commonly filter on derivative variables, i.e., the instantaneous change in some variable with respect to another variable. The two most common derivative variables are speed, the change in position in unit time, and turning angle, the change in heading in unit time. Trajectories must always be checked for correct temporal ordering before calculating metrics with time interval as a denominator. Speed and turning angle filters should be applied in a stepwise fashion, and it is safest to first apply a simple speed filter that removes positions with instantaneous speeds above a certain threshold. This threshold should be informed by the tracked animal’s biology, but it is also important to consider that high frequency tracking typically overestimates the speed between positions due to positioning error (Ranacher et al. 2016; an explanation for animal tracking data is given in Noonan et al. 2019). Even assuming data with large positioning errors have been removed, it is advisable to begin with a liberal speed threshold that excludes only the most unlikely of movements. Speed filters have the drawback of removing at twice as many positions as strictly necessary (the speed duplication problem). For example, in a trajectory of three positions (p1, p2, and p3), the outlier p2 has a high instantaneous speed, as does the ‘real’ position p3, whose displacement is calculated relative to the outlier p2; both will be removed by the same speed filter. Speed filtering with low thresholds also risks losing valid data on fast transit behaviour of a study species, and this is especially a risk for aerial transit which can be strongly assisted by winds. In such cases, turning angle filters can help distinguish between high instantaneous speeds arising from positioning error and those that comprise a fast transit segment. Most animals are physiologically constrained to move quickly only in relatively straight lines, i.e., with low turning angles, and large turning angles coupled with high speeds often represent positioning errors in the data, whereas high speed - low angle positions are likely to capture extreme yet real movements. 3.2 Challenges to speed filters A more serious challenge to speed filters is the tracking system phenomenon of ‘reflected’ positions, in which the estimated location is instantaneously displaced many hundreds of metres from the individual’s real position for some time, followed by a return to a location near the real position. In such a ‘reflection’ comprised of the positions t1…tr, p1…pn, and tr+1…tn where positions denoted by t represent real trajectory positions, and those denoted p represent reflected data a great distance from the real trajectory, the positions p1 and tr+1 will have very high instantaneous speeds. A speed filter applied to such reflected data would only remove p1 and tr+1, leaving the positions p2…pn, which are erroneous and must be removed. Iteratively applying a speed filter to this data while recalculating speeds in each iteration until no extreme speeds remain is an imperfect solution, since it will remove valid positions tr+i for each erroneous position pi; the true scale of the speed duplication problem is thus revealed. Another approach to identifying reflections and especially to determining where they end is to identify the first position with extreme speeds (p1), and calculating the speed between the preceding position (tr) and all subsequent positions (p1…tn) until the first position with a realistic instantaneous speed relative to tr is located; in this case tr+1. The drawback here is that the method is sensitive to the duration of the reflection, since the speed between tr and subsequent (reflected) positions will decrease exponentially to a realistic value over time (as the denominator time increases), and the algorithm will prematurely consider the reflection to have ended. A more heuristic approach to dealing with trajectory reflections involves using the phenomenon’s properties to identify its bounds, and removing all positions between these bounds. The first step is identifying anomalous positions with extreme speeds and turning angles, as in the point outlier case. The trajectory can then be split into pre- and post-anomaly segments, and the first position of the post-anomaly segment, which is the inner bound of the reflection, can be set as an anchor point. The first position after the anchor point with an instantaneous speed &gt;= the anchor point can be considered to be the upper bound of the reflection, and lacking such a position, the position with the next greatest instantaneous speed can be considered the outer bound of the reflection. The positions between these bounds can be identified as a trajectory reflection and removed. To prevent the algorithm from searching through many hundreds of thousands of data points, the algorithm can be set to consider a post-anomaly segment of K positions, and to look for the end of the reflection within this bound. While fast, the drawback with this method is in the simplicity of its assumptions that (1) the first position of the data is correct and not itself a reflection, (2) the reflection ends within K positions, and (3) the individual is not moving towards the reflected positions. Violation of any of these assumptions will cause the method to fail catastrophically: (1) if the first position is a reflection, it will be preserved and real positions will be removed, (2) if the reflection does not end within K positions the algorithm will erroneously assign the highest speed within K positions as the end, and (3) the individual’s real positions overlapping with the reflected positions will result in low speeds between erroneous and real positions, leaving the algorithm unable to find the correct reflection end. If reflections persist in the data after this procedure, or are so prevalent as to cause the algorithm to return erroneous data, there is a limited number of options: (1) attempt to identify whether reflections occur in a common area across individual tracks, and manually identify and remove positions from within the bounds of these so-called ‘attractor points’, (2) subset the data and discard subsets which have more than N anomalous speed-angle positions, as these may indicate many reflected segments, or (3) identify anomalies and remove positions within some time limit of each anomaly, in the hope that the reflection ends by that time. "],
["smoothing-positioning-errors.html", "Section 4 Smoothing positioning errors 4.1 The median smooth 4.2 Other smooths 4.3 Interpolating data", " Section 4 Smoothing positioning errors 4.1 The median smooth While the filters described so far are sufficient to remove serious positioning errors, careful examination of the resulting trajectories may reveal that smaller positioning errors remain – the trajectory appears ‘spiky’ at small scales. These smaller errors are usually more challenging to remove since they have covariates (such as speed and turning angles) which are within the expected range of movement behaviour for the study species, hence their persistence in the data. The high volumes of data generated by high frequency tracking studies can be brought to bear upon this issue, by smoothing the trajectory. A ‘smooth’ works by approximating the true value of an observation based on neighbouring values. A common and easily implemented smoothing technique is nearest neighbour averaging, in which the value of an observation X0 is the average of some K nearest neighbours. The median smooth (confusingly also called the median filter) is a variant of nearest neighbour averaging which replaces X0 with the median of its neighbours, and this is recommended over simple averaging as it is robust to outliers. A median smooth with an appropriate smoothing window (usually 3 – 5 positions) centred on each observation can be applied to the X and Y coordinates of the trajectory to obtain a smoothed track. The median smooth will return as many smoothed coordinates as there are observations in the data, and does not interpolate between positions when data are missing. However, it is robust to even very large gaps, and it is not necessary to split the data into segments separated by periods of missing observations when applying the filter. 4.2 Other smooths 4.3 Interpolating data "],
["reducing-data-volume.html", "Section 5 Reducing data volume 5.1 Thinning animal tracks 5.2 Subsetting on behaviour/env covariates", " Section 5 Reducing data volume 5.1 Thinning animal tracks Most data at this stage is technically ‘clean’, yet its volume alone may pose challenges for older hardware (such as computers or storage) and software (data analysis functions) if these are not optimised for efficient computation. Examples in the R statistical environment favoured by movement ecologists (Joo et al. 2020) include the default function for importing data from a file read.csv, which is much slower than newer alternatives (such as data.table’s fread; Dowle et al. XXXX). This coupled with the requirement of uniform sampling intervals by many methods in the field (e.g. Hidden Markov Models from Michelot et al. 2016, and Step Selection Functions from Signer et al. 2019) makes evenly reducing data volumes worthwhile. The two approaches here are aggregation and resampling, and both begin with rounding the timestamp of the observations to the nearest multiple of the desired interval. When resampling the data, one position for each unique value of the rounded timestamp is retained, while aggregation involves computing the median of values at the positions sharing the same rounded timestamp. While the median aggregation method is less sensitive to position outliers than resampling, it may result in inappropriate fractional values for some position covariates (e.g. the number of base stations in ATLAS data), while others such as positioning error estimates cannot be aggregated using a simple median alone. It is thus advisable to evaluate whether position covariates will be required unaltered in further steps (e.g. positioning error for movement models), in which case resampling may be more appropriate, or whether it is more important to optimise the position estimate, for which aggregation may be better suited. It is natural to consider evenly reducing data volume before implementing filters or smooths, since this is computationally more efficient than thinning after filtering-smoothing. While resampling data will preserve position covariates which may be passed to further filters, it may also sample outliers which could then be discarded by filters, resulting in uneven sampling once more. On the other hand, median aggregation may not correctly handle position covariates on which data can be filtered, rendering further filtering steps suspect. Even assuming median aggregation is applied to data without covariates, increasing the sampling interval by an order of magnitude or more (e.g. from 3 s to 30 s in ATLAS data) will result in significant underestimation of instantaneous speed (see Noonan et al. 2019). This could have knock-on effects on the efficacy of speed filters applied after aggregation, since the instantaneous speeds of point outliers or trajectory reflections may now be within the range of speeds realistically expected for the study species, making these artifacts more challenging to remove. 5.2 Subsetting on behaviour/env covariates Haven’t really spoken about subsetting on behaviour and perhaps this should be merged with the next section which mentions subsetting on speed or other covariates. "],
["synthesising-movement-tracks.html", "Section 6 Synthesising movement tracks 6.1 Challenges in synthesising high-frequency trajectories 6.2 Residence patches and their construction", " Section 6 Synthesising movement tracks 6.1 Challenges in synthesising high-frequency trajectories Having passed through the steps above, animal tracking data are ready to be processed to make biological inferences. Yet these data may still be too large to be handled efficiently by statistical modelling methods without resorting to operations unfamiliar to many biologists, such as parallel processing. Many of these methods attempt to identify correlates of movement behaviour, which can be broadly categorised into correlates of residency, and correlates of movement between residence areas, or more colloquially, why does the animal stay versus why does the animal go. Examples include relating residence time to proxies of resource density, or movement between foraging areas to a proxy of competition. Why residence patches, why other methods are slow etc. WIP 6.2 Residence patches and their construction We propose a simpler alternative which is to synthesise high frequency trajectories into ‘residence patches’ using an intuitive multi-step, multi-scale segmentation and clustering procedure based on first principles/prior knowledge of the species’ biology. The residence patch is defined as a spatio-temporally proximate sequence of consistent positions where the individual was stationary (as in Oudman et al. 2019). While there are sophisticated ways of identifying stationary points in a track (e.g. recursion analysis; Bracis et al. 2018), the simplest method is to subset data with low speeds. The residence patch method uses only three parameters to determine the appropriate clustering. First, the distance between consecutive positions is compared against the buffer radius, which is the maximum distance between two temporally consecutive positions p1 and p2 for them to be considered to be part of the same sequence. Sequences of positions meeting this criterion are clustered into ‘proto-patches’. Next, recognising that proto-patches might represent discontinuous sequences of a larger residence patch (e.g. due to missing data), the time and distance between consecutive proto-patches is compared against the temporal independence limit and the spatial independence limit respectively. Both time difference and distance are calculated from the last position of proto-patch q1 to the first position of the next proto-patch q2. If either of these criteria for independence are met, i.e., proto-patch q2 is located at a distance greater than the spatial independence limit from q1, or begins more than temporal independence limit time units after q1 ends, the two are considered independent. Only q1 takes its final form as a residence patch, while q2 and q3 are then assessed for independence; a proto-patch is always merged with the proto-patch before it if they are found to be non-independent. While this algorithm considers only spatio-temporal differences between proto-patches as criteria for being independent residence patches, it can be easily extended to compare differences in any variable against an appropriate parameter. This comparison may be between variables at the end and beginning of consecutive proto-patches, or it may be between the median values of the proto-patches as a whole. For example, the median residence time (Bracis et al. 2018) of consecutive proto-patches can be very different when individuals switch foraging modes, and this difference can be used to separate them into independent patches despite being spatially and temporally proximate. Having classified the track into residence patches, it is useful to extract summary data which may be passed on to a statistical analysis. Simple metrics of interest to animal ecologists include the patch duration, the distance travelled within the patch, the displacement within the patch, the tortuosity of movement within the patch (the distance – displacement ratio), the displacement between consecutive patches, as well as the median coordinates of the patch. These may be supplemented by more advanced metrics, such as the total patch area (assuming the previously defined buffer radius around each position), and the patch circularity which is a measure of roundness (e.g. Polsby and Popper 1991). The latter spatial metrics require the patch positions to be converted into spatial objects and merged to correct for overlapping buffers. These multi-polygon spatial objects representing each patch can be combined into a convenient spatial object representing the individual’s entire trajectory as a string of residence patches with summary statistics as covariates. Further environmental covariates for these patches are easily extracted from raster or vector spatial data. "]
]
