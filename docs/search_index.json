[
["processing-egyptian-fruit-bat-tracks.html", "Section 7 Processing Egyptian fruit bat tracks 7.1 Prepare libraries 7.2 Read bat data 7.3 Sanity check: Plot bat data 7.4 Prepare data for filtering 7.5 Filter by covariates 7.6 Filter by speed 7.7 Median smoothing 7.8 Making residence patches 7.9 Processed bat patches", " Section 7 Processing Egyptian fruit bat tracks We show the pre-processing pipeline at work on the tracks of three Egyptian fruit bats (Rousettus aegyptiacus), and construct residence patches. 7.1 Prepare libraries # load libs library(data.table) library(RSQLite) library(atlastools) library(ggplot2) library(patchwork) # prepare a palette pal &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;) 7.2 Read bat data Read the bat data and convert to a csv file. # prepare the connection con &lt;- dbConnect(drv = SQLite(), dbname = &quot;data/Three_example_bats.sql&quot;) # list the tables table_name &lt;- dbListTables(con) # prepare to query all tables query &lt;- sprintf(&#39;select * from \\&quot;%s\\&quot;&#39;, table_name) # query the database data &lt;- dbGetQuery(conn = con, statement = query) # disconnect from database dbDisconnect(con) Convert data to csv. # convert data to datatable setDT(data) # write data for QGIS fwrite(data, file = &quot;data/bat_data.csv&quot;) 7.3 Sanity check: Plot bat data Save the raw data plot. Show the plot. 7.4 Prepare data for filtering 7.4.1 Prepare data per individual # split bat data by tag # first make a copy using the data.table function copy # this prevents the orignal data from being modified by atlastools # functions which DO MODIFY BY REFERENCE! data_split &lt;- copy(data) # now split data_split &lt;- split(data_split, data$TAG) 7.5 Filter by covariates No natural bounds suggest themselves, and we proceed to filter by covariates, since point outliers are obviously visible. We use filter out positions with SD &gt; 20 and positions calculated using only 3 base stations. We use the function atl_filter_covariates. # get SD. # since the data are data.tables, no assignment is necessary invisible( lapply(data_split, function(dt) { dt[, SD := sqrt(VARX + VARY + (2 * COVXY))] }) ) # filter for SD &lt;= 20 # here, reassignment is necessary as rows are being removed # the atl_filter_covariates function could have been used here data_split &lt;- lapply(data_split, function(dt) { dt &lt;- atl_filter_covariates( data = dt, filters = c(&quot;SD &lt;= 20&quot;, &quot;NBS &gt; 3&quot;) ) }) # check whether the filter has worked invisible( lapply(data_split, function(dt) { assertthat::assert_that(min(dt$SD) &lt;= 20, msg = &quot;some SDs above 20 remain&quot;) assertthat::assert_that(min(dt$NBS) &gt; 3, msg = &quot;some NBS below 3 remain&quot;) }) ) 7.5.1 Sanity check: Plot filtered data # bind all individuals together data_split &lt;- rbindlist(data_split) Save the filtered data plot. Show the plot. 7.6 Filter by speed Some point outliers remain, and should be removed using a speed filter. First we calculate speeds. # calculate speed on split data once more data_split &lt;- split(data_split, data_split$TAG) # get speeds as with SD, no reassignment required for columns invisible( lapply(data_split, function(dt) { # first process time to seconds # assign to a new column dt[, time := floor(TIME / 1000)] dt[, `:=`(speed_in = atl_get_speed(dt, x = &quot;X&quot;, y = &quot;Y&quot;, time = &quot;time&quot;, type = &quot;in&quot;), speed_out = atl_get_speed(dt, x = &quot;X&quot;, y = &quot;Y&quot;, time = &quot;time&quot;, type = &quot;out&quot;))] }) ) Now filter for speeds &gt; 20 m/s (around 70 km/h). # filter speeds # reassignment is required here data_split &lt;- lapply(data_split, function(dt) { dt &lt;- na.omit(dt, cols = c(&quot;speed_in&quot;, &quot;speed_out&quot;)) dt &lt;- atl_filter_covariates(data = dt, filters = c(&quot;speed_in &lt;= 20&quot;, &quot;speed_out &lt;= 20&quot;)) }) 7.6.1 Sanity check: Plot speed filtered data # bind all individuals together data_split &lt;- rbindlist(data_split) Save the speed filtered data plot. Show the plot. 7.7 Median smoothing Apply a 5 point median smooth to the data. # since the function modifies in place, we shall make a copy data_smooth &lt;- copy(data_split) # split the data again data_smooth &lt;- split(data_smooth, data_smooth$TAG) Remember, atl_median_smooth MODIFIES IN PLACE. # apply the median smooth to each list element # no reassignment is required as THE FUNCTION MODIFIES IN PLACE! invisible( # the function arguments to atl_median_smooth # can be passed directly in lapply lapply(data_smooth, atl_median_smooth, time = &quot;time&quot;, moving_window = 5) ) 7.7.1 Sanity check: Plot smoothed data # recombine split up data that has been smoothed data_smooth &lt;- rbindlist(data_smooth) Save the smoothed data plot. Show the plot. 7.8 Making residence patches 7.8.1 Calculating residence time First, the data is put through the recurse package to get residence time. # load recurse library(recurse) # split the data data_smooth &lt;- split(data_smooth, data_smooth$TAG) Get residence time. Since bats may revisit the same features, we want to prevent confusion between frequent revisits and prolonged residence. For this, we stop summing residence times within Z metres of a location if the animal exited the area for one hour or more. # get residence times data_residence &lt;- lapply(data_smooth, function(dt) { # do basic recurse dt_recurse &lt;- getRecursions( x = dt[, c(&quot;X&quot;, &quot;Y&quot;, &quot;time&quot;, &quot;TAG&quot;)], radius = 50, timeunits = &quot;mins&quot; ) # get revisit stats dt_recurse &lt;- setDT( dt_recurse[[&quot;revisitStats&quot;]] ) # count long absences from the area dt_recurse[, timeSinceLastVisit := ifelse(is.na(timeSinceLastVisit), -Inf, timeSinceLastVisit)] dt_recurse[, longAbsenceCounter := cumsum(timeSinceLastVisit &gt; 60), by = .(coordIdx) ] # get data before the first long absence of 60 mins dt_recurse &lt;- dt_recurse[longAbsenceCounter &lt; 1, ] dt_recurse &lt;- dt_recurse[, list( resTime = sum(timeInside), fpt = first(timeInside), revisits = max(visitIdx) ), by = .(coordIdx, x, y) ] # prepare and merge existing data with recursion data dt[, coordIdx := seq(nrow(dt))] dt &lt;- merge(dt, dt_recurse[, c(&quot;coordIdx&quot;, &quot;resTime&quot;)], by = c(&quot;coordIdx&quot;)) setorderv(dt, &quot;time&quot;) }) 7.8.2 Sanity check: Residence-time time-series # bind the list data_residence &lt;- rbindlist(data_residence) # get time as human readable data_residence[, ts := as.POSIXct(time, origin = &quot;1970-01-01&quot;)] Save the speed filtered data plot. Show the plot. 7.8.3 Constructing residence patches Split the data and construct residence patches. Some preparation is required. First, the function requires columns x, y, time, and id. # add an id column data_residence[, `:=`(id = TAG, x = X, y = Y)] # filter for residence time &gt; 5 minutes data_residence &lt;- data_residence[resTime &gt; 5, ] # split the data data_residence &lt;- split(data_residence, data_residence$TAG) Now segment-cluster into residence patches. # segment into residence patches data_patches &lt;- lapply(data_residence, atl_res_patch, buffer_radius = 25) 7.8.4 Getting residence patch data We get the residence patch data as spatial sf-MULTIPOLYGON objects. # get data spatials data_spatials &lt;- lapply(data_patches, atl_patch_summary, which_data = &quot;spatial&quot;, buffer_radius = 25) # bind list data_spatials &lt;- rbindlist(data_spatials) # convert to sf library(sf) data_spatials &lt;- st_sf(data_spatials, sf_column_name = &quot;polygons&quot;) # assign a crs st_crs(data_spatials) &lt;- st_crs(2039) 7.8.5 Write patch spatial representations st_write(data_spatials, dsn = &quot;data/data_bat_residence_patches.gpkg&quot;) Write cleaned bat data. data_clean &lt;- fwrite(rbindlist(data_smooth), file = &quot;data/data_bat_smooth.csv&quot;) Write patch summary. # get summary patch_summary &lt;- lapply(data_patches, atl_patch_summary) # bind summary patch_summary &lt;- rbindlist(patch_summary) # write fwrite(patch_summary, &quot;data/data_bat_patch_summary.csv&quot;) 7.9 Processed bat patches This figure made in QGIS. "]
]
