[
["index.html", "A Rough Guide to Pre-processing High-Frequency Animal Tracking Data Section 1 Abstract", " A Rough Guide to Pre-processing High-Frequency Animal Tracking Data Pratik R. Gupte and others 2020-10-08 Section 1 Abstract Data cleaning is a ubiquitous pre-processing step in analysis pipelines, and its automation is essential for large data volumes such as those generated in animal tracking studies using high-frequency Time-of-Arrival (TOA) systems. Users of systems such as ATLAS must contend with two intertwined data cleaning challenges: (1) reducing positioning errors, and (2) the high volume of data itself. Making biological inferences from data with positioning errors is not straightforward, and processing large data volumes is computationally intensive. Though reducing positioning error is widely recommended, users are without uniform guidance on how to go about this, and lack a common set of computationally efficient tools. Further, many methods that synthesize movement tracks for ecological inference are either (1) not suited to very large datasets, or (2) not intuitive to understand in terms of the tracked animal’s biology. In this article we introduce a pipeline to pre-process high-frequency animal tracking data in order to prepare it for subsequent analysis. We demonstrate this pipeline on simulated movement data to which we have randomly added positioning errors. This pipeline is suited to any tracking study in which the high data volume combined with knowledge of the tracked individuals’ biology can be used to smooth out positioning errors. We further suggest how large volumes of cleaned data may be synthesized into biologically meaningful ‘residence patches’, and demonstrate how this accurately captures animal space-use. Finally, we introduce the R package atlastools which provides fast implementations of the methods we describe. Though aimed at ATLAS systems, atlastools can be used with any time-series animal movement data, and we demonstrate its usage with both simulated and empirical examples. "],
["getting-started.html", "Section 2 Getting started 2.1 Installing atlastools 2.2 Simulating movement data", " Section 2 Getting started This section covers: Installing the R package atlastools, Simulating some realistic looking movement data using the R package smoove from (Gurarie et al. 2017), and Introducing positioning errors into the simulated movement data. 2.1 Installing atlastools This paper refers extensively to the R package atlastools (Gupte 2020), which can be installed from Github. Releases of the package from Github can be found on Zenodo: The build status of the ‘Master’ branch is shown here: The code chunk below shows how to install atlastools. # use either devtools or remotes to install install.packages(&quot;devtools&quot;) # installation using devtools devtools::install_github(&quot;pratikunterwegs/atlastools&quot;) 2.2 Simulating movement data Here, we simulate some movement data using the smoove R package from (Gurarie et al. 2017). First, we load smoove and data.table, as well as some helper functions that make use of them to simulate data for use. # load smoove and datatable library(smoove) library(data.table) # source helper functions source(&quot;R/helper_functions.R&quot;) data &lt;- do_smoove_data() # save simulated data fwrite(data, &quot;data/data_sim.csv&quot;) References "],
["reducing-large-scale-positioning-error-by-filtering-data.html", "Section 3 Reducing Large-Scale Positioning Error by Filtering Data 3.1 Prepare Libraries 3.2 Introducing Errors to the Data 3.3 Filtering by Spatial Bounds 3.4 Filtering Unrealistic Movement", " Section 3 Reducing Large-Scale Positioning Error by Filtering Data 3.1 Prepare Libraries Here we load some useful libraries, and the helper functions. # to handle movement data library(data.table) library(atlastools) # to plot library(ggplot2) library(patchwork) # source helper functions source(&quot;R/helper_functions.R&quot;) 3.2 Introducing Errors to the Data Here we introduce three kinds of errors to the data: Small-scale normally distributed errors at each position; Large-scale error at a random 0.5% of positions; A large-scale displacement of a sequence of 300 positions. While the data are 10,000 positions at 1-second interval, we shall use only 5,000 of these. # read in the data data &lt;- fread(&quot;data/data_sim.csv&quot;)[5000:10000, ] We add outliers at random to the data to demonstrate their removal. # make a copy data_copy &lt;- copy(data) # add a prolonged spike or reflection to 300 positions data_copy[500:800, `:=`(x = x + 0.25, y = y + 0.25)] # add normal error data_copy[, `:=`(x = do_add_error(x, std_dev = 0.01), y = do_add_error(y, std_dev = 0.005))] # add 100 outliers data_copy &lt;- do_add_outliers(data_copy, p_data = 0.005, std_dev = 0.1) Save the data to which errors have been added. fwrite(data_copy, file = &quot;data/data_errors.csv&quot;) Define a palette with 4 colours for convenience. # define a four colour palette pal &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;) We make a figure of the canonical data (grey line) along with the artificially added error (grey points). # make figure of canonical data with added errors figure_raw &lt;- ggplot()+ geom_point(data = data_copy, aes(x, y), col = &quot;grey&quot;, alpha = 1, size = 0.2)+ geom_path(data = data, aes(x, y), col = &quot;grey20&quot;, alpha = 1)+ ggthemes::theme_map()+ coord_equal()+ theme(axis.text = element_blank(), axis.title = element_blank(), legend.position = &quot;none&quot;)+ theme(plot.background = element_rect(fill = NA))+ labs(colour = NULL) 3.3 Filtering by Spatial Bounds Filtering by spatial bounds is a good way to begin reducing gross, large-scale positioning errors. There are two main ways of doing this: Filtering by a bounding box: Compare the coordinates of observations against a range of acceptable coordinates, and retain those which fall within the range, or Filtering by a spatial polygon: An explicit geometric intersection between the positions and a polygon representing an area of interest. Here, we remove gross positioning errors using a bounding box filter using the function atl_filter_bounds. In this example, we show filtering only on the Y coordinate. atl_filter_bounds takes coordinate ranges as two-element vectors of the lower and higher bound. It is possible to pass one of the bounds as NA, in which case, only the other bound is used for filtering, i.e., y_range = c(NA, 1) is equivalent to selecting all Y coordinates &lt; 1. atl_filter_bounds was initially designed to remove positions inside a specific range, hence the argument remove_inside. The default value of the argument is FALSE, and atl_filter_bounds is thus a bounding box filter. # remove positions outside a bounding box # NB: set remove_inside to FALSE data_inside_bbox &lt;- atl_filter_bounds(data = data_copy, y_range = c(0.5, 1), remove_inside = FALSE) atl_filter_bounds is not vectorised, and if there are two or more bounds per coordinate (for instance, \\(X_1 \\ldots X_2\\), and \\(X_3 \\ldots X_4\\)), they must be passed in two different function calls. The same is true for filtering by an sf polygon. atl_filter_bounds also supports filtering by multi-polygon objects. Having filtered the data we shall prepare it for plotting. # plot data inside and outside bbox fig_filter_bounds &lt;- ggplot()+ geom_point(data = data_inside_bbox, aes(x, y), col = pal[2], alpha = 1, size = 0.2)+ geom_point(data = data_copy[!data_inside_bbox, on = c(&quot;x&quot;, &quot;y&quot;)], aes(x, y), col = pal[1], alpha = 0.5, size = 0.2)+ geom_path(data = data, aes(x, y), col = &quot;grey20&quot;, alpha = 1)+ geom_hline(yintercept = c(0.5, 1), col = &quot;grey&quot;, lty = 2)+ ggthemes::theme_map()+ theme(plot.background = element_rect(fill = NA))+ coord_equal() # wrap plots plot_figure &lt;- wrap_plots(list(figure_raw, fig_filter_bounds)) + plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) &amp; theme(plot.tag = element_text(face = &quot;bold&quot;)) ggsave(plot_figure, filename = &quot;figures/fig_raw_bounds.png&quot;, width = 170, height = 170, units = &quot;mm&quot;) (a) A velocity-autocorrelated movement track simulated for 5,000 positions (black line) using the smoove package (Gurarie et al. 2017). Three kinds of errors have been artificially added: (1) each position (grey points) is offset from the canonical track with the addition of normally distributed small-scale error, (2) large-scale error has been added to 0.5% of positions, and (3) 300 positions (indices 500 – 800) have been displaced to the top-right of the track to simulate a gross distortion that affects a continuous subset of the track. The goal of pre-processing such datasets is to get the estimated positions (grey points) to match the canonical track (solid black line) as closely as possible. (b) Tracks can be quickly filtered by spatial bounds (\\(0.5 \\leq Y \\leq 1.0\\); dashed grey lines) using the atlastools function atl_filter_bounds. Setting the function argument remove_inside = FALSE retains positions within user supplied bounds (blue points), and excludes those outside (red points). 3.3.1 A Note on Filtering by Bounds The filter on spatial bounds is only for demonstration, and is not applied to the data. 3.4 Filtering Unrealistic Movement Large-scale positioning error can affect both point locations as well as entire subsets of a track, leading to a track appearing to show unrealistic movement for the study species (Bjørneraas et al. 2010). The best way to examine whether a track contains such positioning errors is to plot the data, and especially to join the dots, i.e., to connect the positions with lines rather than simply plot points. Users with extensive experience of their study system will readily recognise segments of a track where the movement appears to be unrealistic. Briefly, the two main kinds of errors are mentioned here, and named based on their appearance in a tack: those that affect single positions lend a spiky appearance to a track, and are referred to as point outliers or spikes, while errors affecting continuous subsets of a track cause it to appear as though it has been reflected along a plane, and are hence called reflections, or prolonged spikes. 3.4.1 Filtering Point Outliers or Spikes We begin by removing unrealistic movement in the form of point outliers, or spikes. The first step is to determine how spikes should be identified. The non-movement approach prescribes determining whether movement metrics associated with each position are realistic or not, and targeting those positions where the movement metrics are unrealistic for the study species. In this case we shall calculate only two metrics, speed and turning angle. These are conveniently implemented in atlastools using the functions atl_get_speed and atl_turning_angle. # get speed and turning angle data_copy[, `:=`(in_speed = atl_get_speed(data_copy, type = &quot;in&quot;), out_speed = atl_get_speed(data_copy, type = &quot;out&quot;), angle = atl_turning_angle(data_copy))] Having calculated speed and turning angle, the next step is to remove positions with extremely high incoming and outgoing speeds. This means using the atl_filter_covariates function to remove positions with speed \\(\\geq\\) a plausible speed threshold. The use of a turning angle filter is optional, and not necessary in this case. One approach is to define a speed cutoff based on expert knowledge, and this is best suited to well studied species. For simulated data, there is no plausible speed threshold from prior knowledge — a reasonable choice here is to use the 90^th or 95^th percentile of speed and turning angle (when this metric is used). This is also more general than identifying the limits of implausibility for each individual (since there may be inter-individual differences), let alone each species in a large dataset. # get 90 and 95 percentile of speed and turning angle sapply(data_copy[, c(&quot;in_speed&quot;, &quot;angle&quot;)], function(z) { quantile(z, probs = c(0.9, 0.95), na.rm = TRUE) }) Finally, we shall remove positions whose incoming and outgoing speeds are both greater than the 95th speed percentile using atl_filter_covariates. # filter the copy by the 95th percentile data_filtered &lt;- atl_filter_covariates(data_copy, filters = c(&quot;(in_speed &lt; 0.024 &amp; out_speed &lt; 0.024) | angle &lt; 40&quot;)) We prepare the data for plotting. # data plot fig_outlier_remove &lt;- ggplot()+ geom_path(data = data_copy, aes(x, y), col = &quot;grey&quot;, lwd = 0.2)+ geom_point(data = data_copy[500:800, ], aes(x, y), shape = 2, col = pal[4])+ geom_point(data = data_copy[!data_copy[500:800, ], on = c(&quot;x&quot;, &quot;y&quot;)], aes(x, y, col = (in_speed &gt;= 0.03 &amp; out_speed &gt;= 0.03), shape = (in_speed &gt;= 0.03 &amp; out_speed &gt;= 0.03)), show.legend = F, alpha = 0.5)+ geom_path(data = data, aes(x, y), col = &quot;grey20&quot;, alpha = 1)+ geom_path(data = data, aes(x, y), col = &quot;grey20&quot;, alpha = 1)+ ggthemes::theme_map()+ scale_color_brewer(palette = &quot;Set1&quot;, direction = -1)+ scale_shape_manual(values = c(1, 19))+ coord_equal()+ theme(plot.background = element_rect(fill = NA)) 3.4.2 Filtering Reflections or Track Subsets When entire track subsets are affected by positioning error, they are more difficult to remove. This is the case when positions are reflected along a plane in the coordinate axis — the simplest way in which this can be explained is to examine the way in which we simulated the reflection earlier in this text, by simply adding an offset to the X and Y coordinates of each of 300 consecutive positions (indices 500 – 800). An explanation of why these reflections occur is found in WATLAS PAPER: Bijleveld et al. in prep. In short, they are results of an error in the ATLAS localisation algorithm, and are usually removed in the localisation step’s quality control procedures. However, some reflections may remain to confront users. Reflections and other issues affecting track subsets cannot be resolved by targeting single positions, since there are rarely any position-specific covariates that can be used to identify a position as part of a larger subset that should be removed. However, it is often possible to identify the bounds of problematic subsets, such as reflections, and remove positions between them. The way this is done is conceptually similar to the point outlier algorithm. The atl_remove_reflection function implements one method remove reflected subsets of a track. The working is described: Remove point outliers, Re-calculate speed and turning angles Identify the first unrealistic movement position (fast speed and high turning angle) Setting this point as an anchor, identify the next position with unrealistic movement (as above) Remove all positions between these two points, Search for the next unrealistic movement, and repeat the process. This is a minimal algorithm which can be developed further, and iterated multiple times to comprehensively remove reflections. One important function argument is the estimated reflection length, i.e., how many positions are estimated to be reflected. This argument controls how many positions after the anchor are candidates for the reflection’s end. If the reflection does not end within this number of positions, the algorithm goes awry, and considers part of the reflection to be the valid data. Fortunately, this can be avoided by setting the length to the number of observations in the data. However, if the reflection does not end at all, the algorithm will simply delete all positions from the anchor point onwards. Thus, the algorithm is less of a double-edged sword and more a sword with no hilt — there are few safe ways to use it. The difficulty in implementing a reasonable algorithm for dealing with this kind of track-subset wide positioning error reveals the general problem that algorithms are not easy to conceptualise or efficiently implement. # attempt to remove reflections data_no_reflection &lt;- atl_remove_reflections(data_with_reflection, point_angle_cutoff = 10, reflection_speed_cutoff = 0.024) # get reflections reflection &lt;- data_with_reflection[!data_no_reflection, on = c(&quot;x&quot;, &quot;y&quot;)] reflection &lt;- na.omit(reflection) # get plots fig_reflection &lt;- ggplot()+ geom_path(data = reflection, aes(x, y), alpha = 1, col = &quot;grey&quot;)+ geom_point(data = reflection, aes(x, y), alpha = 0.5, col = pal[1], shape = 2)+ geom_point(data = data_no_reflection, aes(x, y), alpha = 0.5, colour = pal[2], shape = 1, show.legend = F)+ geom_path(data = data, aes(x, y), col = &quot;grey20&quot;, alpha = 1)+ ggthemes::theme_map()+ coord_equal()+ theme(plot.background = element_rect(fill = NA)) # wrap figures plot_figure &lt;- wrap_plots(list(fig_outlier_remove, fig_reflection)) + plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) &amp; theme(plot.tag = element_text(face = &quot;bold&quot;)) plot_figure ggsave(plot_figure, filename = &quot;figures/fig_correct_tracks.png&quot;, width = 170, height = 170, units = &quot;mm&quot;) Reducing large-scale positioning error in a movement track. (a) Positioning error may affect single observations resulting in point outliers or ‘spikes’ (red points), but it may also affect continuous susbets of a track, which we call a prolonged spike' (purple triangles). While the former may be targeted by filtering on appropriate covariates such as speed and turning angle using theatlastoolsfunctionatl_filter_covariates, the latter cannot be effectively corrected by targeting each coordinate pair in isolation. **(b)** Theatlastoolsfunctionatl_remove_reflections` to identify prolonged spikes (red triangles) in tracking data is an illustrative example of targeting positioning errors that affect track subsets. While this method returns the canonical track without the prolonged spike (blue points) in this example, users are cautioned to frequently check this and similar semi-supervised algorithms’ results. Finally, we export the data without spikes and reflections to be used later. fwrite(data_no_reflection, file = &quot;data/data_no_reflection.csv&quot;) References "],
["reducing-small-scale-positioning-error-and-aggregating-data.html", "Section 4 Reducing Small-Scale Positioning Error and Aggregating Data 4.1 Preparing Libraries and Data 4.2 Median Smoothing 4.3 Thinning Data by Aggregation 4.4 Aggregation Before Reducing Positioning Errors", " Section 4 Reducing Small-Scale Positioning Error and Aggregating Data Once large-scale positioning errors have been removed using filters on bounds or on unrealistic movement, small-scale positioning errors may remain. Why is this the case? The simple reason is that high-frequency tracking has a large component of error relative to the real movement of an individual, and the error / movement ratio increases with the frequency of tracking. (Noonan et al. 2019) has a better explanation, which users are strongly recommended to read. The effect of these small-scale errors for high-frequency data is to cause overestimations of straight-line displacement (distance between two positions) and speed. If users were to apply a very strict speed filter, many of these positions would be lost. This loss of information would certainly reduce the sampling frequency, which would lead to better estimates of total distance and mean speed. However, while total distance and mean speed estimates would improve, the instantaneous distance and speed estimates would remain biased, which will most likely lead to erroneous inferences from the data. However, users can turn the high-frequency of their data against itself by applying a simple smooth. The median smooth is recommended because it is robust to outliers, and is very well understood. 4.1 Preparing Libraries and Data We first load some useful packages and define a colour palette. # prep libs library(data.table) library(atlastools) library(ggplot2) library(patchwork) # prepare a palette pal &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;) We load both the simulated, canonical data (which has no errors), as well as the data with errors which has been pre-processed to remove large-scale errors. We also assign a variable, window_size, which will help identify the datasets when comparing median smooth moving window (\\(K\\)) sizes. # read in the data and set the window size variable data &lt;- fread(&quot;data/data_sim.csv&quot;)[5000:10000, ] data[, window_size := NA] # data with small scale errors but no reflections or outliers data_errors &lt;- fread(&quot;data/data_no_reflection.csv&quot;) data_errors[, window_size := 0] 4.2 Median Smoothing We apply a median smooth with four different values of \\(K\\), the moving window size — 3, 5, 11, and 21 positions. \\(K\\) must be an odd number, but apart from that there is no correct magnitude of \\(K\\). Very large \\(K\\) will lead to unrealistic tracks, while small \\(K\\) will not result in much reduction of error. Users are encouraged to plot their data before and after smoothing to examine the effect of different window sizes. The atlastools function atl_median_smooth is quite fast and users can readily try multiple \\(K\\) values as shown in the example below. # smooth the data over four K values list_of_smooths &lt;- lapply(c(3, 5, 11, 21), function(K) { data_copy &lt;- copy(data_errors) data_copy &lt;- atl_median_smooth(data = data_copy, x = &quot;x&quot;, y = &quot;y&quot;, time = &quot;time&quot;, moving_window = K) data_copy[, window_size := K] }) 4.2.1 Robustness to Large Gaps The median smooth is robust to large gaps in the data, and tracks do not need to be split when a large chunk of data are missing. The easiest way to illustrate this is with a figure. # make a single list of data data_plot &lt;- append(list(data_errors), list_of_smooths) # now prep smooth over Y coordinate fig_smooth_1d &lt;- ggplot()+ geom_point(data = rbindlist(data_plot), aes(time, y, col = factor(window_size)), alpha = 0.5, size = 1, show.legend = FALSE, shape = 16)+ coord_cartesian(xlim = c(5000, 7000), ylim = c(0.7, NA), expand = F)+ scale_colour_brewer(palette = &quot;Spectral&quot;)+ ggthemes::theme_few()+ theme(axis.text = element_blank())+ labs(x = &quot;position index&quot;, y = &quot;Y coordinate&quot;) We prepare a plot of the smoothed data as well as the demonstration of the method relative to gaps. # prepare data to plot # make list of data to plot figure_median_smooth &lt;- mapply(function(df, col) { ggplot()+ geom_point(data = df, aes(x, y), size = 0.5, alpha = 0.5, show.legend = FALSE, col = col, shape = 1)+ geom_path(data = data, aes(x, y), col = &quot;grey20&quot;)+ coord_cartesian(expand = T, ylim = c(0.6, 0.8)) + ggthemes::theme_map() }, data_plot, c(pal[2], rep( RColorBrewer::brewer.pal(3, &quot;Greens&quot;)[2], 4)), SIMPLIFY = F) # make basic plot with rectangle fig_bounds &lt;- ggplot()+ geom_point(data = data_errors, aes(x, y), size = 0.1, alpha = 0.2, col = &quot;grey&quot;)+ geom_path(data = data, aes(x, y), size = 0.1)+ geom_hline(yintercept = c(0.6, 0.8), col = &quot;grey20&quot;, lty = 2, lwd = 0.2)+ coord_equal(expand = F) + ggthemes::theme_map() # append to first figure figure_median_smooth[[1]] &lt;- figure_median_smooth[[1]]+ annotation_custom(grob = ggplotGrob(fig_bounds), ymin = 0.6, ymax = 0.8, xmin = 0.5, xmax = 0.7) # wrap plots fig_median_smooth &lt;- wrap_plots(append(figure_median_smooth, list(fig_smooth_1d)), ncol = 3)+ plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) &amp; theme(plot.tag = element_text(face = &quot;bold&quot;)) # save figure ggsave(fig_median_smooth, filename = &quot;figures/fig_median_smooth.png&quot;, width = 225, height = 225 / 1.77, units = &quot;mm&quot;) 4.3 Thinning Data by Aggregation Evenly thinning data is a good idea if statistical methods require even sampling, or if the volume of data is too large for statistical packages to efficiently handle it. In R, both may be true at once. Here, we demonstrate thinning by aggregation on data that has been median smoothed using a \\(K\\) of 11. # choose the 11 point median smooth data data_agg &lt;- copy(list_of_smooths[[3]]) # get list of aggregated data list_of_agg &lt;- lapply(c(3, 10, 30, 120), function(z) { data_return &lt;- atl_thin_data(data = data_agg, interval = z, method = &quot;aggregate&quot;) data_return[, interval := z] return(data_return) }) We shall prepare a figure showing the aggregation steps. # make figure of aggregation figure_aggregate &lt;- mapply(FUN = function(df) { ggplot(data = df)+ geom_point(data = list_of_smooths[[3]], aes(x, y), col = pal[3], size = 0.3, alpha = 0.5)+ geom_point(aes(x, y, size = SD), colour = pal[4], alpha = 1, stroke = 1, shape = 4, show.legend = F)+ geom_path(aes(x, y), colour = pal[4], alpha = 1)+ geom_path(data = data, lwd = 0.2, aes(x, y), col = &quot;grey20&quot;)+ coord_equal(ylim = c(0.6, 0.8))+ ggthemes::theme_few()+ theme(axis.title = element_blank(), axis.text = element_blank()) }, list_of_agg[2:3], SIMPLIFY = FALSE) figure_aggregate &lt;- wrap_plots(figure_aggregate)+ plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) &amp; theme(plot.tag = element_text(face = &quot;bold&quot;)) # save figure ggsave(figure_aggregate, filename = &quot;figures/fig_aggregate_data.png&quot;, width = 185, height = 90, units = &quot;mm&quot;) 4.4 Aggregation Before Reducing Positioning Errors Users may rightly wonder whether they can get away with aggregating their data, using a median aggregation function, and reduce data volumes, correct uneven sampling frequency, and reduce large-scale errors all in one go. The answer to most questions in ecology is, “It depends”. Median aggregation before correcting positioning errors can indeed be advantageous; for instance for faster visualisation while preserving the broad structure of a track. However, there are drawbacks. The main one is that information is lost in the aggregation process, lending less power to steps such as smoothing applied after aggregation. Further, aggregation (and indeed any kind of thinning) results in significantly different estimates of speed and distance from the real speed. We show the effect of aggregating before any error correction here. # read data with errors data_errors &lt;- fread(&quot;data/data_errors.csv&quot;) # aggregate before correction list_of_agg &lt;- lapply(c(3, 10, 30, 120), function(z) { data_return &lt;- atl_thin_data(data = data_errors, interval = z, method = &quot;aggregate&quot;) data_return[, interval := z] data_return[, speed := atl_get_speed(data_return)] return(data_return) }) # get real speed data[, speed := atl_get_speed(data)] We prepare the figures. ### plot figures fig_agg_data &lt;- lapply(list_of_agg, function(df) { ggplot(df)+ geom_path(aes(x,y), # col = factor(interval), # group = interval), lwd = 0.2)+ geom_point(aes(x,y, size = SD, shape = speed &lt; quantile(data$speed, 0.95, na.rm = T), col = speed &lt; quantile(data$speed, 0.95, na.rm = T), group = interval), stroke = ifelse(df$speed &lt; quantile(data$speed, 0.95, na.rm = T), 1, 1), show.legend = F)+ geom_path(data = data, aes(x, y), lwd = 0.1)+ scale_shape_manual(values = c(2, 4))+ scale_colour_manual(values = pal[c(1, 2)])+ coord_cartesian(ylim = c(0.6, NA))+ ggthemes::theme_map() }) # now plot distribution of speed data_agg &lt;- rbindlist(list_of_agg) # show boxplot of speed fig_agg_speed &lt;- ggplot(data_agg)+ geom_jitter(aes(factor(interval), speed), col = &quot;grey&quot;, shape = 4, alpha = 0.5)+ geom_boxplot(aes(factor(interval), speed), fill = NA, show.legend = F, width = 0.3, outlier.size = 0.2)+ geom_hline(yintercept = c(mean(data$speed, na.rm = T), quantile(data$speed, na.rm = T, probs = c(0.95)) ), lty = c(1, 2))+ scale_y_log10(label = scales::comma)+ ggthemes::theme_few()+ theme(axis.text.y = element_blank())+ labs(x = &quot;interval (s)&quot;, y = &quot;speed&quot;) # make combined figure fig_aggregate &lt;- wrap_plots(append(fig_agg_data[2:3], list(fig_agg_speed)), design = &quot;AAABBBCC&quot;)+ plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) &amp; theme(plot.tag = element_text(face = &quot;bold&quot;)) # save figure ggsave(fig_aggregate, filename = &quot;figures/fig_aggregate_errors.png&quot;, width = 170, height = 170 / 1.67, units = &quot;mm&quot;) Thinning a movement track using median aggregation preserves track structure, but affects essential track metrics such as speed. (a, b) Movement tracks with a canonical interval of 1s aggregated over intervals of (a) 10 and (b) 30 seconds, without removing large- or small-scale positioning errors. All symbols represent positions in the aggregated track, with the size of the symbol representing the standard deviation at each position. Blue crosses represent positions with speed \\(\\leq\\) the 95^th percentile of canonical speeds, while red triangles represent positions with speed \\(\\geq\\) 95^th percentile of canonical speeds. (c) Boxplot of instantaneous speeds after median aggregation of a 1s interval track over intervals of 3, 10, 30, and 120 seconds, but without the removal of positioning errors. The mean and 95^th percentile of speed in the canonical track are shown as solid and dashed lines, respectively. Aggregation without reducing positioning errors can result in speed estimates that are substantially different from the true speed. References "],
["residence-patches-and-their-construction.html", "Section 5 Residence patches and their construction Prepare libraries 5.1 An example with simulated data", " Section 5 Residence patches and their construction Prepare libraries library(data.table) library(atlastools) library(ggplot2) # prepare a palette pal &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;) 5.1 An example with simulated data 5.1.1 Read data and classify # read in the data data &lt;- fread(&quot;data/data_sim.csv&quot;)[5000:10000, ] data[, id := &quot;a&quot;] # make residence patch patch &lt;- atl_res_patch(data, buffer_radius = 0.0001, lim_spat_indep = 0.02, lim_time_indep = 3) # get spatial representation patch_sf &lt;- atl_patch_summary(patch_data = patch, which_data = &quot;spatial&quot;, buffer_radius = 0.01) # get summary data patch_summary &lt;- atl_patch_summary(patch_data = patch, which_data = &quot;summary&quot;) 5.1.2 Plot classified residence patches plot_patches &lt;- ggplot()+ geom_sf(data = patch_sf, colour = &quot;grey20&quot;, fill = &quot;grey90&quot;, show.legend = FALSE)+ geom_point(data = patch_summary, aes(x_median + 0.5, y_median, size = duration), # size = 8, shape = 2, col = pal[2], stroke = 2, show.legend = F)+ geom_path(data = data, aes(x, y), size = 0.3)+ geom_curve(data = patch_summary, aes(x = x_end + 0.5, y = y_end, xend = shift(patch_summary$x_start, type = &quot;lag&quot;) + 0.5, yend = shift(patch_summary$y_start, type = &quot;lag&quot;)), col = &quot;grey30&quot;, size = 0.3, ncp = 2, curvature = -0.4, arrow = arrow(angle = 20, length = unit(2, units = &quot;mm&quot;), type = &quot;closed&quot;, ends = &quot;first&quot;))+ annotate(geom = &quot;text&quot;, x = c(0.5, 1.1), y = 1, label = sprintf(&quot;(%s)&quot;, c(&quot;a&quot;, &quot;b&quot;)), fontface = &quot;bold&quot;)+ scale_colour_distiller(palette = &quot;YlGnBu&quot;, trans = &quot;log10&quot;)+ coord_sf()+ ggthemes::theme_map()+ theme(legend.position = &quot;right&quot;) plot_patches # save figure ggsave(plot_patches, filename = &quot;figures/fig_residence_patch.png&quot;, width = 170, height = 100, units = &quot;mm&quot;) "],
["processing-calibration-data.html", "Section 6 Processing calibration data 6.1 Prepare libraries 6.2 Preliminary visualisation 6.3 Filter by bounding box 6.4 Filter trajectories 6.5 Smoothing the trajectory 6.6 Making residence patches 6.7 Compare patch metrics", " Section 6 Processing calibration data 6.1 Prepare libraries # load libs library(data.table) library(atlastools) library(ggplot2) library(patchwork) # prepare a palette pal &lt;- RColorBrewer::brewer.pal(4, &quot;Set1&quot;) 6.2 Preliminary visualisation # read and plot example data data &lt;- fread(&quot;data/atlas1060_allTrials_annotated.csv&quot;) data_raw &lt;- copy(data) # plot data fig_data_raw &lt;- ggplot(data)+ geom_path(aes(x, y), col = &quot;grey&quot;, alpha = 0.5, size = 0.3)+ geom_point(aes(x, y), col = &quot;steelblue&quot;, alpha = 0.2, size = 2)+ ggthemes::theme_map()+ coord_sf(crs = 32631) # save figure ggsave(fig_data_raw, filename = &quot;figures/fig_calibration_raw.png&quot;, width = 185 / 25) 6.3 Filter by bounding box Save an unprocessed copy. data_unproc &lt;- copy(data) Filter by a bounding box. # remove inside must be set to falses data &lt;- atl_filter_bounds(data = data, x = &quot;x&quot;, y = &quot;y&quot;, x_range = c(645000, max(data$x)), remove_inside = FALSE) Plot the result. # plot data fig_data_bbox &lt;- ggplot()+ geom_path(data = data_raw, aes(x, y), col = &quot;grey&quot;, alpha = 0.5, size = 0.3)+ geom_point(data = data_raw, aes(x, y, col = x &gt; 645000), alpha = ifelse(data_raw$x &gt; 645000, 0.5, 1), size = ifelse(data_raw$x &gt; 645000, 0.3, 2), show.legend = F)+ scale_color_brewer(palette = &quot;Set1&quot;)+ geom_vline(xintercept = 645000, col = &quot;grey&quot;, lty = 2)+ ggspatial::annotation_scale(location = &quot;br&quot;)+ ggthemes::theme_map()+ coord_sf(crs = 32631) # save result ggsave(fig_data_bbox, filename = &quot;figures/fig_calib_bbox.png&quot;, width = 185 / 25) 6.3.1 Remove low base station positions # save a copy data_unproc &lt;- copy(data) # remove NBS &lt;= 3 data &lt;- atl_filter_covariates(data = data, filters = &quot;NBS &gt; 3&quot;) fig_low_nbs &lt;- ggplot()+ geom_path(data = data, aes(x, y), col = &quot;grey&quot;, alpha = 0.5, size = 0.3)+ geom_point(data = data_unproc, aes(x, y, col = as.factor(NBS &gt; 3)), shape = 1, alpha = 0.5, size = 0.3, show.legend = F)+ scale_color_brewer(palette = &quot;Set1&quot;)+ ggthemes::theme_map()+ ggspatial::annotation_scale(location = &quot;tl&quot;)+ coord_sf(crs = 32631)+ labs(colour = &quot;NBS&quot;) # save figure ggsave(fig_low_nbs, filename = &quot;figures/fig_low_nbs.png&quot;, width = 185 / 25, height = 185 / 25) 6.4 Filter trajectories 6.4.1 Handle time Time in ATLAS tracking is counted in milliseconds and is represented by a 64-bit integer (type long), which is not natively supported in R; it will instead be converted to a numeric, or double. This is not what is intended, but it works. The bit64 package can help handle 64-bit integers if you want to keep to intended type. A further issue is that 64-bit integers (whether represented as bit64 or double) do not yield meaninful results when you try to convert them to a date-time object, such as of the class POSIXct. This is because as.POSIXct fails when trying to work with 64-bit integers (it cannot interpret this type), and returns a date many thousands of years in the future (approx. 52,000 CE) if the time column is converted to numeric. There are two possible solutions. The parsimonious one is to convert the 64-bit number to a 32-bit short integer (dividing by 1000), or to use the nanotime package. The conversion method loses an imperceptible amount of precision. The nanotime requires installing another package. The first method is shown here. In the spirit of not destroying data, we create a second lower-case column called time. # divide by 1000, convert to integer, then convert to POSIXct data[, time := as.POSIXct(as.integer(TIME / 1000), origin = &quot;1970-01-01&quot;)] 6.4.2 Add speed and turning angle # add incoming and outgoing speed data[, `:=` (speed_in = atl_get_speed(data, x = &quot;x&quot;, y = &quot;y&quot;, time = &quot;time&quot;), speed_out = atl_get_speed(data, type = &quot;out&quot;))] # add turning angle data[, angle := atl_turning_angle(data = data)] 6.4.3 Get 95th percentile of speed and angle # use sapply speed_angle_thresholds &lt;- sapply(data[, list(speed_in, speed_out, angle)], quantile, probs = 0.9, na.rm = T) 6.4.4 Plot to see speeds # plot filtered data fig_speed_outliers &lt;- ggplot()+ geom_path(data = data, aes(x, y), col = &quot;grey&quot;, alpha = 0.5, size = 0.3)+ geom_point(data = data, aes(x, y, col = speed_in &lt; 15.7), size = 0.3, alpha = 0.5, show.legend = F)+ scale_color_brewer(palette = &quot;Set1&quot;)+ ggthemes::theme_map()+ ggspatial::annotation_scale(location = &quot;tl&quot;)+ coord_sf(crs = 32631)+ labs(colour = &quot;speed (m/s)&quot;) # save ggsave(fig_speed_outliers, filename = &quot;figures/fig_speed_outlier.png&quot;, width = 185 / 25, height = 185 / 25) 6.4.5 Filter on speed Here we use a speed threshold of 20 m/s, 4 m/s lower than the 95th percentile. # make a copy data_unproc &lt;- copy(data) # remove speed outliers data &lt;- atl_filter_covariates(data = data, filters = c(&quot;(speed_in &lt; 15.7 &amp; speed_out &lt; 15.7)&quot;)) # recalculate speed and angle data[, `:=` (speed_in = atl_get_speed(data, x = &quot;x&quot;, y = &quot;y&quot;, time = &quot;time&quot;), speed_out = atl_get_speed(data, type = &quot;out&quot;))] # add turning angle data[, angle := atl_turning_angle(data = data)] 6.5 Smoothing the trajectory # apply a 5 point median smooth, first make a copy data_unproc &lt;- copy(data) # now apply the smooth data &lt;- atl_median_smooth(data = data, x = &quot;x&quot;, y = &quot;y&quot;, time = &quot;time&quot;, moving_window = 5) # make figure fig_smooth &lt;- ggplot()+ geom_path(data = data, aes(x, y), col = RColorBrewer::brewer.pal(4, &quot;Set1&quot;)[4])+ annotate(geom = &quot;rect&quot;, xmin = c(650785, 650045), ymin = c(5904450, 5902700), xmax = c(652500, 650682), ymax = c(5906133, 5903080), fill = &quot;grey90&quot;, alpha = 0.5, col = NA)+ annotate(geom = &quot;text&quot;, x = c(650785, 650045) + 400, y = c(5904450, 5902700) + 300, col = &quot;black&quot;, label = c(&quot;d&quot;, &quot;e&quot;))+ theme_void()+ coord_sf(crs = 32631)+ labs(colour = &quot;track&quot;) # make zoomed in figures fig_inset &lt;- mapply(function(xl, yl) { ggplot()+ geom_point(data = data_raw[!data_unproc, on = c(&quot;x&quot;, &quot;y&quot;)], aes(x, y), col = &quot;grey&quot;, shape = 4, size = 0.4)+ geom_point(data = data_unproc, aes(x, y), col = RColorBrewer::brewer.pal(3, &quot;Set1&quot;)[3], shape = 1, alpha = 0.5)+ geom_path(data = data, aes(x, y), col = RColorBrewer::brewer.pal(4, &quot;Set1&quot;)[4])+ ggthemes::theme_few()+ theme(axis.title = element_blank(), axis.text = element_blank())+ ggspatial::annotation_scale(location = &quot;br&quot;)+ coord_cartesian( xlim = xl, ylim = yl, expand = F )+ labs(colour = &quot;track&quot;) }, list(xl1 = c(650785, 652500), xl2 = c(650045, 650682)), list(yl1 = c(5904450, 5906133), yl2 = c(5902700, 5903080)), SIMPLIFY = FALSE) # add global figure to second inset fig_inset[[2]] &lt;- fig_inset[[2]] + annotation_custom(grob = ggplotGrob(fig_smooth), xmin = 649990, xmax = 650045 + (650682 - 650045)/2, ymin = 5903080 - (5903080 - 5902700)/2.5, ymax = 5903080 ) fig_median_smooth &lt;- wrap_plots(fig_inset)+ plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) # save figure ggsave(filename = &quot;figures/fig_calib_median_smooth.png&quot;, width = 185 / 25, height = 90 / 25) 6.5.1 Plot pre-processing steps # make combined walkthrough figure figure_walkthrough &lt;- wrap_plots( append( list( fig_data_bbox, fig_low_nbs, fig_speed_outliers), fig_inset), design = &quot;ABD\\nACE&quot;)+ plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) # save combined figure ggsave(figure_walkthrough, filename = &quot;figures/fig_walkthrough.png&quot;, height = 130, width = 225, units = &quot;mm&quot;) 6.6 Making residence patches 6.6.1 Prepare data An indicator of individual residence at or near a position can be useful when attempting to identify residence patches. Positions can be filtered on a metric such as residence time (see Bracis et al. 2018). In this dataset, residence positions are marked in the tID column as beginning with WP. These can be extracted and converted into residence patches. # make a data copy data_unproc &lt;- copy(data) # assign residence positions data[, residence := stringi::stri_detect(tID, regex = &quot;WP&quot;)] # make a copy of only residence points data_res &lt;- data[residence == T, ] 6.6.2 Calculate residence time # load recurse library(recurse) # get recurse data for a 10m radius data_recurse &lt;- getRecursions(data_unproc[, list(x, y, time, TAG)], radius = 200) 6.6.3 Run residence patch method # on known residence points patch_res_known &lt;- atl_res_patch(data_res, buffer_radius = 10, lim_spat_indep = 100, lim_time_indep = 30, min_fixes = 3) 6.6.4 Get spatial and summary objects # for the known and unkniwn patches patch_sf_data &lt;- atl_patch_summary(patch_res_known, which_data = &quot;spatial&quot;) # assign crs sf::st_crs(patch_sf_data) &lt;- 32631 # get summary data patch_summary_data &lt;- atl_patch_summary(patch_res_known, which_data = &quot;summary&quot;) 6.6.5 Plot the outcome # patch with residence points and all patches fig_basic_residence &lt;- ggplot()+ geom_path(data = data, aes(x, y), lwd = 0.2, col = &quot;grey20&quot;)+ geom_point(data = data_res, aes(x, y), shape = 2, size = 2, col = pal[2])+ geom_path(data = patch_summary_data, aes(x_median + 200, y_median), col = &quot;orange&quot;, lty = 1)+ geom_point(data = patch_summary_data, aes(x_median + 200, y_median, fill = duration / 60), size = 5, shape = 21)+ scale_fill_distiller(palette = &quot;YlOrRd&quot;, direction = 1, trans = &quot;log10&quot;, limits = c(1, 60))+ ggthemes::theme_map()+ theme(legend.position = c(0, 0.3), legend.key.width = unit(3, units = &quot;mm&quot;), legend.background = element_blank())+ ggspatial::annotation_scale(location = &quot;tl&quot;)+ coord_sf(crs = 32631)+ labs(fill = &quot;inferred\\nduration (min)&quot;) 6.7 Compare patch metrics 6.7.1 Compare known patches # get known patch summary patch_summary_real &lt;- data_res[, list(nfixes_real = .N, x_median = round(median(x), digits = -2), y_median = round(median(y), digits = -2), duration_real = as.numeric( max(time) - min(time))), by = &quot;tID&quot;] # round median coordinate for inferred patches patch_summary_inferred &lt;- patch_summary_data[, c(&quot;x_median&quot;, &quot;y_median&quot;, &quot;nfixes&quot;, &quot;duration&quot;) ][, `:=`(x_median = round(x_median, digits = -2), y_median = round(y_median, digits = -2))] # join with respatch summary patch_summary_compare &lt;- merge(patch_summary_real, patch_summary_inferred, on = c(&quot;x_median&quot;, &quot;y_median&quot;), all.x = TRUE, all.y = TRUE) 6.7.2 Plot durations comparisons # get linear model model_duration &lt;- lm(duration_real ~ duration, data = patch_summary_compare) # get R2 summary(model_duration) # make figure comparing different methods figure_patch_duration &lt;- ggplot()+ geom_segment(data = patch_summary_compare, aes(x = 100, xend = duration, y = duration_real, yend = duration_real), col = &quot;grey&quot;)+ geom_segment(data = patch_summary_compare, aes(x = duration, xend = duration, y = 100, yend = duration_real), col = &quot;grey&quot;)+ geom_point(data = patch_summary_compare, aes(duration, duration_real), show.legend = F, size = 3, stroke = 1, shape = 21, fill = pal[2], col = &quot;white&quot;)+ geom_smooth(data = patch_summary_compare, aes(duration, duration_real), se = F, size = 0.5, col = pal[1], alpha = 0.2, method = &quot;glm&quot;, show.legend = F)+ annotate(geom = &quot;text&quot;, x = 300, y = 900, size = 5, label = &quot;R^2 == 0.849&quot;, parse = T)+ scale_x_log10(breaks = c(300, 600, 900), labels = as.integer(c(300, 600, 900) / 60))+ scale_y_log10(breaks = c(300, 600, 900), labels = as.integer(c(300, 600, 900) / 60))+ coord_equal(expand = F, xlim = c(200, 1200), ylim = c(200, 1200))+ ggthemes::theme_few()+ labs(x = &quot;inferred duration (min)&quot;, y = &quot;real duration (min)&quot;) 6.7.3 Join patch map and duration comparison figure # wrap together figure_res_patch &lt;- wrap_plots(list(fig_basic_residence, figure_patch_duration))+ plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) &amp; theme(plot.tag = element_text(face = &quot;bold&quot;)) # save figure ggsave(figure_res_patch, filename = &quot;figures/fig_calib_residence_patch.png&quot;, height = 130, width = 225, units = &quot;mm&quot;) "],
["references.html", "Section 7 References", " Section 7 References "]
]
