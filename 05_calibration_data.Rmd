---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Validating the Residence Patch Method with Calibration Data

Here we show how the residence patch method [@barraquand2008; @bijleveld2016; @oudman2018] accurately estimates the duration of known stops in a track collected as part of a calibration exercise in the Wadden Sea.

## Outline of Cleaning Steps

We begin by preparing the libraries we need, and installing `atlastools` from Github.
After installing `atlastools`, we visualise the data to check for location errors, and find a single outlier position approx. 15km away from the study area (Fig. 1.1, 1.2).
This outlier is removed by filtering data by the X coordinate bounds using the function `atl_filter_bounds`; X coordinate bounds $\leq$ 645,000 in the UTM 31N coordinate reference system were removed (n = 1; remaining positions = 50,815; Fig. 1.2).
We then calculate the incoming and outgoing speed, as well as the turning angle at each position using the functions `atl_get_speed` and `atl_turning_angle` respectively, as a precursor to targeting large-scale location errors in the form of point outliers.
We use the function `atl_filter_covariates` to remove positions with incoming and outgoing speeds $\geq$ the speed threshold of 15 m/s (n = 13,491, 26.5%; remaining positions = 37,324, 73.5%; Fig. 1.3; main text Fig. 7.b).
This speed threshold is chosen as the fastest boat speed during the experiment, 15 m/s.
Finally, we target small-scale location errors by applying a median smoother with a moving window size $K$ = 5 using the function `atl_median_smooth` (Fig. 1.4; main text Fig. 7.c).
Smoothing does not reduce the number of positions.
We thin the data to a 30 second interval leaving 1,803 positions (4.8\% positions of the smoothed track)

## Prepare libraries

First we prepare the libraries we need. Libraries can be installed from CRAN if necessary.

```{r}
# for data handling
library(data.table)
library(atlastools)

# for recursion analysis
library(recurse)

# for plotting
library(ggplot2)
library(patchwork)

# making a colour palette
pal <- RColorBrewer::brewer.pal(5, "Set1")
pal[3] <- "seagreen"
```

## Install `atlastools` from Github

`atlastools` is available from Github and is archived on Zenodo [@gupte2020a].
It can be installed using `remotes` or `devtools`. Here we use the `remotes` function `install_github`.

```{r install_atlastools}
install.packages("remotes")

# installation using remotes
remotes::install_github("pratikunterwegs/atlastools")
```

## Access data and preliminary visualisation

First we access the data from a local file using the `data.table` package [@dowle2020].
We then visualise the raw data.

```{r}
# read and plot example data
data <- fread("data/atlas1060_allTrials_annotated.csv")
data_raw <- copy(data)
```

Here we show how data can be easily visualised using the popular plotting package `ggplot2`.
Note that we plot both the points (`geom_point`) and the inferred path between them (`geom_path`), and specify a geospatial coordinate system in metres, suitable for the Dutch Wadden Sea (UTM 31N; ESPG code:32631; `coord_sf`).
We save the output to file for future reference.

Since plot code can become very lengthy and complicated, we omit showing further plot code in versions of this document rendered as PDF or HTML; it can however be seen in the online `.Rmd` version.

```{r echo=TRUE}
# plot data
fig_data_raw <-
  ggplot(data) +
  geom_path(aes(x, y),
    col = "grey", alpha = 1, size = 0.2
  ) +
  geom_point(aes(x, y),
    col = "grey", alpha = 0.2, size = 0.2
  ) +
  ggthemes::theme_few() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank()
  ) +
  coord_sf(crs = 32631)

# save figure
ggsave(fig_data_raw,
  filename = "figures/fig_calibration_raw.png",
  width = 185 / 25
)
```

![The raw data from a calibration exercise conducted around the island of Griend in the Dutch Wadden Sea. A handheld WATLAS tag was used to examine how ATLAS data compared to GPS tracks, and we use the WATLAS data here to demonstrate the basics of the pre-processing pipeline, as well as validate the residence patch method. It is immediately clear from the figure that the track shows location errors, both in the form of point outliers as well as small-scale errors around the true location.](figures/fig_calibration_raw.png)

## Filter by bounding box

We first save a copy of the data, so that we can plot the raw data with the cleaned data plotted over it for comparison.

```{r}
# make a copy using the data.table copy function
data_unproc <- copy(data)
```

We then filter by a bounding box in order to remove the point outlier to the far south east of the main track. We use the `atl_filter_bounds` functions using the `x_range` argument, to which we pass the limit in the UTM 31N coordinate reference system.
This limit is used to exclude all points with an X coordinate < 645,000.

We then plot the result of filtering, with the excluded point in black, and the points that are retained in green.

```{r}
# remove inside must be set to falses
data <- atl_filter_bounds(
  data = data,
  x = "x", y = "y",
  x_range = c(645000, max(data$x)),
  remove_inside = FALSE
)
```

```{r echo=FALSE}
# plot data
fig_data_bbox <-
  ggplot() +
  geom_path(
    data = data_raw,
    aes(x, y),
    col = "grey", alpha = 0.5, size = 0.3
  ) +
  geom_point(
    data = data_raw,
    aes(x, y,
      col = x > 645000
    ),
    alpha = ifelse(data_raw$x > 645000, 1, 1),
    size = ifelse(data_raw$x > 645000, 0.3, 4),
    shape = ifelse(data_raw$x > 645000, 16, 4),
    show.legend = F
  ) +
  scale_colour_manual(values = c(pal[1], pal[3])) +
  geom_vline(
    xintercept = 645000,
    col = "grey", lty = 2
  ) +
  ggspatial::annotation_scale(location = "br") +
  ggthemes::theme_few() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  coord_sf(crs = 32631)

# save result
ggsave(fig_data_bbox,
  filename = "figures/fig_calib_bbox.png",
  width = 185 / 25
)
```

![Removal of a point outlier using the function `atl_filter_bounds`. The point outlier (black point) is removed based on its X coordinate value, with the data filtered to exclude positions with an X coordinate < 645,000 in the UTM 31N coordinate system. Positions that are retained are shown in green.](figures/fig_calib_bbox.png)

## Filter trajectories

### Handle time

Time in ATLAS tracks is represented by 64-bit integers (type `long`) that specify time in milliseconds, starting from the beginning of 1970 (the UNIX epoch). This representation of time is called `POSIX` time and is usually specified in seconds, not milliseconds. 

Since about 1.6 billion seconds have passed since the beginning of 1970, current `POSIX` times in milliseconds cannot be represented by R's built-in 32-bit integers. A naive conversion results in truncation of out-of-range numbers leading to huge errors (dates many thousands of years in the future).

R does not natively support 64-bit integers. One option is to use the bit64 package, which adds 64-bit integer support to R. 

A simpler solution is to convert the times to R's built in `double` data type (also called `numeric`), which uses a 64-bit floating point representation. This representation can represent integers with up to 16 digits without error; we only need 13 digits to represent the number of milliseconds since 1970, so the conversion is error free. We can also perform the conversion and then divide by 1000 so that times are represented in seconds, not milliseconds; this simplifies speed estimation.

If second-resolution is accurate enough (it is for our purposes), the solution that we use is to divide times by 1000 to reduce the resolution from milliseconds to seconds and then to convert the time stamps to R integers.
In the spirit of not destroying data, we create a second lower-case column called `time` to store this 

```{r}
# divide by 1000, convert to integer, then convert to POSIXct
data[, time := as.integer(
  as.numeric(TIME) / 1000
)]
```

### Add speed and turning angle

```{r}
# add incoming and outgoing speed
data[, `:=`(
  speed_in = atl_get_speed(data,
    x = "x",
    y = "y",
    time = "time"
  ),
  speed_out = atl_get_speed(data, type = "out")
)]

# add turning angle
data[, angle := atl_turning_angle(data = data)]
```

### Get 95th percentile of speed and angle

```{r}
# use sapply
speed_angle_thresholds <-
  sapply(data[, list(speed_in, speed_out, angle)],
    quantile,
    probs = 0.9, na.rm = T
  )
```

```{r echo=FALSE}
# plot filtered data
fig_speed_outliers <-
  ggplot() +
  geom_point(
    data = data,
    aes(x, y,
      col = speed_in < 25.6
    ),
    size = 0.3, alpha = 0.5,
    show.legend = F
  ) +
  annotate(
    geom = "rect",
    xmin = 650785, xmax = 653250,
    ymin = 5904450, ymax = 5906133,
    fill = NA, col = "grey20"
  ) +
  annotate(
    geom = "rect",
    ymin = 5901650, ymax = 5903100,
    xmin = 650000, xmax = 650600,
    fill = NA, col = "grey20"
  ) +
  scale_color_manual(values = c("grey", pal[3])) +
  ggthemes::theme_few() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  ggspatial::annotation_scale(location = "tl") +
  coord_sf(crs = 32631)
# save
ggsave(fig_speed_outliers,
  filename = "figures/fig_speed_outlier.png",
  width = 170 / 25, height = 170 / 25
)
```

### Filter on speed

Here we use a speed threshold of 15 m/s, the fastest known boat speed.
We then plot the data with the extreme speeds shown in grey, and the positions retained shown in green.

```{r}
# make a copy
data_unproc <- copy(data)

# remove speed outliers
data <- atl_filter_covariates(
  data = data,
  filters = c("(speed_in < 15 & speed_out < 15)")
)

# recalculate speed and angle
data[, `:=`(
  speed_in = atl_get_speed(data,
    x = "x",
    y = "y",
    time = "time"
  ),
  speed_out = atl_get_speed(data, type = "out")
)]

# add turning angle
data[, angle := atl_turning_angle(data = data)]
```

![Improving data quality by filtering out positions that would require unrealistic movement. We removed positions with speeds $\geq$ 15 m/s, which is the fastest possible speed in this calibration data, part of which was collected in a moving boat around Griend. Grey positions are removed, while green positions are retained. Rectangles indicate areas expanded for visualisation in following figures.](figures/fig_speed_outlier.png)

## Smoothing the trajectory

We then apply a median smooth over a moving window ($K$ = 5).
This function modifies in place, and does not need to be assigned to a new variable.
We create a copy of the data before applying the smooth so that we can compare the data before and after smoothing.

```{r}
# apply a 5 point median smooth, first make a copy
data_unproc <- copy(data)

# now apply the smooth
atl_median_smooth(
  data = data,
  x = "x", y = "y", time = "time",
  moving_window = 5
)
```

```{r, echo=FALSE}
# make zoomed in figures
fig_smooth <-
  ggplot() +
  geom_path(
    data = data_raw[!data_unproc, on = c("x", "y")],
    aes(x, y),
    col = "grey90",
    size = 0.1
  ) +
  geom_point(
    data = data_raw[!data_unproc, on = c("x", "y")],
    aes(x, y),
    col = "grey",
    shape = 4,
    size = 0.4
  ) +
  geom_point(
    data = data,
    aes(x, y),
    col = pal[3],
    shape = 1,
    stroke = 1,
    alpha = 0.5
  ) +
  coord_cartesian(
    xlim = c(650785, 653250),
    ylim = c(5904450, 5906133),
    expand = F
  ) +
  ggthemes::theme_few() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank()
  ) +
  ggspatial::annotation_scale(location = "br")

ggsave(fig_smooth,
  filename = "figures/fig_calib_median_smooth.png",
  width = 90 / 25, height = 90 / 25
)
```

![Reducing small-scale location error using a median smooth with a moving window $K$ = 5. Median smoothed positions are shown in green, while raw, unfiltered data is shown in grey. Median smoothing successfully recovers the likely path of the track without a loss of data. The area shown is the upper rectangle from Fig. 1.3.](figures/fig_calib_median_smooth.png)

## Thinning the data

Next we thin the data to demonstrate thinning by median smoothing.
Following this, we plot the median smooth and thinning by aggregation.

```{r}
# save a copy
data_unproc <- copy(data)

# remove columns we don't need
data <- data[, setdiff(
  colnames(data),
  c("tID", "Timestamp", "id", "TIME", "UTCtime")
),
with = FALSE
]

# thin to a 30s interval
data_thin <- atl_thin_data(
  data = data,
  interval = 30,
  method = "aggregate",
  id_columns = "TAG"
)
```


```{r echo=FALSE}
# make zoomed in figures
fig_smooth_thin <-
  ggplot() +
  geom_path(
    data = data_raw[!data_unproc, on = c("x", "y")],
    aes(x, y),
    col = "grey90",
    size = 0.1
  ) +
  geom_point(
    data = data_raw[!data_unproc, on = c("x", "y")],
    aes(x, y),
    col = "grey",
    shape = 4,
    size = 0.4
  ) +
  geom_point(
    data = data_thin,
    aes(x, y, size = count),
    col = pal[4],
    alpha = 0.8,
    shape = 0, show.legend = F
  ) +
  geom_point(
    data = data_unproc,
    aes(x, y),
    col = pal[3],
    shape = 16
  ) +
  scale_size(range = c(2, 6)) +
  coord_cartesian(
    xlim = c(650785, 653250),
    ylim = c(5904450, 5906133),
    expand = F
  ) +
  ggthemes::theme_few() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank()
  ) +
  ggspatial::annotation_scale(location = "br")

ggsave(fig_smooth_thin,
  filename = "figures/fig_calib_smooth_thin.png",
  width = 90 / 25, height = 90 / 25, dpi = 300
)
```

```{r echo=FALSE}
# make combined walkthrough figure
figure_walkthrough <-
  wrap_plots(
    list(
      fig_speed_outliers, fig_smooth_thin
    ),
    design = "AB"
  )

# # save combined figure
ggsave(figure_walkthrough,
  filename = "figures/fig_walkthrough.png",
  height = 90, width = 170, units = "mm"
)
```

![Thinning by aggregation over a 30 second interval (down from 1 second) preserves track structure while reducing the data volume for computation. Here, thinned positions are shown as purple squares, with the size of the square indicating the number of positions within the 30 second bin used to obtain the average position. Green points show the median smoothed data from Fig. 1.4, while the raw data are shown in grey. The area shown is the upper rectangle in Fig. 1.3.](figures/fig_calib_smooth_thin.png)

## Residence patches

### Get waypoint centroids

We subset the annotated calibration data to select the waypoints and the positions around them which are supposed to be the locations of known stops. Since each stop was supposed to be 5 minutes long, there are multiple points in each known stop.

```{r}
library(stringi)
data_res <- data_unproc[stri_detect(tID, regex = "(WP)")]
```

From this data, we get the centroid of known stops, and determine the time difference between the first and last point within 50 metres, and within 10 minutes of the waypoint positions' median time.

Essentially, this means that the maximum duration of a stop can be 20 minutes, and stops above this duration are not expected.

```{r}
# get centroid
data_res_summary <- data_res[, list(
  x_median = median(x),
  y_median = median(y),
  t_median = median(time)
),
by = "tID"
]

# now get times 10 mins before and after
data_res_summary[, `:=`(
  t_min = t_median - (10 * 60),
  t_max = t_median + (10 * 60)
)]

# make a list of positions 10min before and after
wp_data <- mapply(function(l, u, mx, my) {
  tmp_data <- data_unproc[inrange(time, l, u)]
  tmp_data[, distance := sqrt((mx - x)^2 + (my - y)^2)]

  # keep within 50
  tmp_data <- tmp_data[distance <= 50, ]

  # get duration
  return(diff(range(tmp_data$time)))
}, data_res_summary$t_min, data_res_summary$t_max,
data_res_summary$x_median, data_res_summary$y_median,
SIMPLIFY = TRUE
)
```

### Prepare data

An indicator of individual residence at or near a position can be useful when attempting to identify residence patches. Positions can be filtered on a metric such as residence time [@bracis2018].

### Calculate residence time

First we calculate the residence time with a radius of 50 metres.
For this, we need a dataframe with coordinates, the timestamp, and the animal id.
We save this data to file for later use.

```{r}
# get 4 column data
data_for_patch <- data_thin[, list(x, y, time, TAG)]

# get recurse data for a 10m radius
recurse_stats <- getRecursions(data_for_patch,
  radius = 50, timeunits = "mins"
)

# assign to recurse data
data_for_patch[, res_time := recurse_stats$residenceTime]

# save recurse data
fwrite(data_for_patch, file = "data/data_calib_for_patch.csv")
```

### Run residence patch method

We subset data with a residence time > 5 minutes in order to construct residence patches.
From this subset, we construct residence patches using the parameters: `buffer_radius` = 5 metres, `lim_spat_indep` = 50 metres, `lim_time_indep` = 5 minutes, and `min_fixes` = 3.

```{r}
# assign id as tag
data_for_patch[, id := as.character(TAG)]

# on known residence points
patch_res_known <- atl_res_patch(data_for_patch[res_time >= 5, ],
  buffer_radius = 5,
  lim_spat_indep = 50,
  lim_time_indep = 5,
  min_fixes = 3
)
```

### Get spatial and summary objects

We get spatial and summary ouput of the residence patch method using the `atl_patch_summary` function using the options `which_data` = "spatial" and `which_data` = "summary.
We use a buffer radius here of 20 metres for the spatial buffer, despite using a buffer radius of 5 metres earlier, simply because it is easier to visualise in the output figure.

```{r}
# for the known and unkniwn patches
patch_sf_data <- atl_patch_summary(patch_res_known,
  which_data = "spatial",
  buffer_radius = 20
)

# assign crs
sf::st_crs(patch_sf_data) <- 32631

# get summary data
patch_summary_data <- atl_patch_summary(patch_res_known,
  which_data = "summary"
)
```

### Prepare to plot data

We read in the island's shapefile to plot it as a background for the residence patch figure.

```{r}
# read griend and hut
griend <- sf::st_read("data/griend_polygon/griend_polygon.shp")
hut <- sf::st_read("data/griend_hut.gpkg")
```

```{r echo=FALSE}
# patch with residence points and all patches
fig_basic_residence <-
  ggplot() +
  geom_sf(data = griend, fill = "antiquewhite") +
  geom_point(
    data = data_res_summary,
    aes(x_median, y_median),
    size = 10, 
    shape = 2,
    stroke = 1,
    col = pal[4],
    alpha = 1
  ) +
  geom_sf(
    data = hut,
    size = 10, shape = 2,
    stroke = 1,
    col = pal[1]
  ) +
  geom_path(
    data = data_for_patch,
    aes(x, y),
    show.legend = F,
    size = 0.3,
    col = "grey"
  ) +
  geom_point(
    data = data_for_patch,
    aes(x, y),
    show.legend = F,
    shape = 1, col = "grey50"
  ) +
  geom_sf(
    data = patch_sf_data[patch_sf_data$duration >= 5 * 60, ],
    fill = pal[3],
    col = pal[3],
    alpha = 0.3
  ) +
  scale_colour_manual(values = c("grey50", pal[3])) +
  scale_shape_manual(values = c(4, 19)) +
  ggspatial::annotation_scale(location = "tl") +
  ggthemes::theme_few() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.background = element_rect(fill = "aliceblue")
  ) +
  coord_sf(
    crs = 32631,
    expand = F,
    ylim = c(5901650, 5903100),
    xlim = c(650000, 650600)
  )
```

```{r echo=FALSE}
ggsave(fig_basic_residence,
  filename = "figures/fig_calib_residence.png",
  height = 170, width = 80, units = "mm"
)
```

![Classifying thinned data into residence patches yields robust estimates of the duration of known stops. The island of Griend (53.25$^{\circ}$N, 5.25$^{\circ}$E) is shown in beige. Residence patches (green polygons; function parameters in text) correspond well to the locations of known stops (purple crossed-squares). However, the algorithm identified all areas with prolonged residence, including those which were not intended stops (n = 12; green polygons without crossed-squares). The algorithm also failed to find two stops of 6 and 15 seconds duration, since these were lost in the data thinning step (crossed-square without green polygon shows one of these). The area shown is the lower rectangle in Fig. 1.3.](figures/fig_calib_residence.png)

## Compare patch metrics

We then merge the annoated, known stop data with the calculated patch duration.
We filter this data to exclude one exceedingly long outlier of about an hour (WP080), which how

```{r}
# get known patch summary
data_res <- data_unproc[stringi::stri_detect(tID, regex = "(WP)"), ]

# get waypoint summary
patch_summary_real <- data_res[, list(
  nfixes_real = .N,
  x_median = round(median(x), digits = -2),
  y_median = round(median(y), digits = -2)
),
by = "tID"
]

# add real duration
patch_summary_real[, duration_real := wp_data]

# round median coordinate for inferred patches
patch_summary_inferred <-
  patch_summary_data[
    ,
    c(
      "x_median", "y_median",
      "nfixes", "duration", "patch"
    )
  ][, `:=`(
    x_median = round(x_median, digits = -2),
    y_median = round(y_median, digits = -2)
  )]

# join with respatch summary
patch_summary_compare <-
  merge(patch_summary_real,
    patch_summary_inferred,
    on = c("x_median", "y_median"),
    all.x = TRUE, all.y = TRUE
  )

# drop nas
patch_summary_compare <- na.omit(patch_summary_compare)

# drop patch around WP080
patch_summary_compare <- patch_summary_compare[tID != "WP080", ]
```

7 patches are identified where there are no waypoints, while 2 waypoints are not identified as patches. These waypoints consisted of 6 and 15 (WP098 and WP092) positions respectively, and were lost when the data were aggregated to 30 second intervals.

### Linear model durations

We run a simple linear model.

```{r}
# get linear model
model_duration <- lm(duration_real ~ duration,
  data = patch_summary_compare
)

# get R2
summary(model_duration)

# write to file
writeLines(
  text = capture.output(
    summary(model_duration)
  ),
  con = "data/model_output_residence_patch.txt"
)
```

```{r echo=FALSE}
# make figure comparing different methods
figure_lm_duration <-
  ggplot() +
  geom_point(
    data = patch_summary_compare,
    aes(
      duration,
      duration_real
    ),
    show.legend = F,
    size = 2, col = "grey"
  ) +
  geom_smooth(
    data = patch_summary_compare,
    aes(
      duration,
      duration_real
    ),
    se = F,
    col = pal[1],
    lwd = 0.5,
    method = "lm",
    show.legend = F
  ) +
  annotate(
    geom = "text",
    x = 600, y = 1020,
    size = 5,
    label = "R^2 == 0.908",
    parse = T, family = "Arial"
  ) +
  scale_x_continuous(
    breaks = 60 * seq(3, 18, 2),
    labels = seq(3, 18, 2)
  ) +
  scale_y_continuous(
    breaks = 60 * seq(5, 20, 2),
    labels = seq(5, 20, 2)
  ) +
  coord_fixed(
    expand = T,
    ratio = 1.115
    #             # xlim = c(400, 1050),
    #             ylim = c(250, 1050)
  ) +
  geom_abline(
    slope = 1, lty = 2,
    col = "black"
  ) +
  ggthemes::theme_few() +
  labs(
    x = "inferred duration (min)",
    y = "real duration (min)"
  )

ggsave(figure_lm_duration,
  filename = "figures/fig_calib_lm_duration.png",
  width = 80 / 25, height = 80 / 25
)
```

![The inferred duration of residence patches corresponds very closely to the real duration (grey circles, red line shows linear model fit), with an underestimation of the true duration of around 2%. The dashed black line represents $y = x$ for reference.](figures/fig_calib_lm_duration.png)

### Linear model summary

```{r, eval=TRUE}
cat(
  readLines(
    con = "data/model_output_residence_patch.txt",
    encoding = "UTF-8"
  ),
  sep = "\n"
)
```

## Plot figure 7

Plotting code is not shown in PDF and HTML form, see the `.Rmd` file.

```{r echo=FALSE}
# wrap together
figure_res_patch <-
  wrap_plots(list(
    fig_speed_outliers,
    fig_smooth_thin,
    fig_basic_residence
  ),
  design = "AC\nAC\nBC\nBC"
  ) +
    plot_annotation(
      tag_levels = "a",
      tag_prefix = "(",
      tag_suffix = ")"
    ) &
    theme(plot.tag = element_text(face = "bold"))

# save figure
ggsave(figure_res_patch,
  filename = "figures/fig_06_calib_residence_patch.png",
  height = 170, width = 170, units = "mm"
)
```
