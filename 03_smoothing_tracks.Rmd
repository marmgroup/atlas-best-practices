---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Reducing Small-Scale Positioning Error and Aggregating Data

Once large-scale positioning errors have been removed using filters on bounds or on unrealistic movement, small-scale positioning errors may remain. Why is this the case?

The simple reason is that high-frequency tracking has a large component of error relative to the real movement of an individual, and the error / movement ratio increases with the frequency of tracking. [@noonan2019] has a better explanation, which users are strongly recommended to read.

The effect of these small-scale errors for high-frequency data is to cause overestimations of straight-line displacement (distance between two positions) and speed. If users were to apply a very strict speed filter, many of these positions would be lost. This loss of information would certainly reduce the sampling frequency, which would lead to better estimates of total distance and mean speed. However, while _total_ distance and _mean_ speed estimates would improve, the instantaneous distance and speed estimates would remain biased, which will most likely lead to erroneous inferences from the data.

However, users can turn the high-frequency of their data against itself by applying a simple smooth. The median smooth is recommended because it is robust to outliers, and is very well understood.

## Preparing Libraries and Data

We first load some useful packages and define a colour palette.

```{r}
# prep libs
library(data.table)
library(atlastools)
library(ggplot2)
library(patchwork)

# prepare a palette
pal <- RColorBrewer::brewer.pal(4, "Set1")
```

We load both the simulated, canonical data (which has no errors), as well as the data with errors which has been pre-processed to remove large-scale errors.

We also assign a variable, `window_size`, which will help identify the datasets when comparing median smooth moving window ($K$) sizes.

```{r read_sim_data_2}
# read in the data and set the window size variable
data <- fread("data/data_sim.csv")[5000:10000, ]
data[, window_size := NA]

# data with small scale errors but no reflections or outliers
data_errors <- fread("data/data_no_reflection.csv")
data_errors[, window_size := 0]
```

## Median Smoothing

We apply a median smooth with four different values of $K$, the moving window size --- 3, 5, 11, and 21 positions.
$K$ must be an odd number, but apart from that there is no correct magnitude of $K$.
Very large $K$ will lead to unrealistic tracks, while small $K$ will not result in much reduction of error.
Users are encouraged to plot their data before and after smoothing to examine the effect of different window sizes.

The `atlastools` function `atl_median_smooth` is quite fast and users can readily try multiple $K$ values as shown in the example below.

```{r}
# smooth the data over four K values
list_of_smooths <- lapply(c(3, 5, 11, 21), function(K) {
  
  data_copy <- copy(data_errors)
  
  data_copy <- atl_median_smooth(data = data_copy,
                    x = "x", 
                    y = "y",
                    time = "time",
                    moving_window = K)
  
  data_copy[, window_size := K]
})
```

### Robustness to Large Gaps

The median smooth is robust to large gaps in the data, and tracks do not need to be split when a large chunk of data are missing.
The easiest way to illustrate this is with a figure.

```{r}
# make a single list of data
data_plot <- append(list(data_errors), list_of_smooths)

# now prep smooth over Y coordinate
fig_smooth_1d <-
  ggplot()+
  geom_point(data = rbindlist(data_plot),
             aes(time, y,
                 col = factor(window_size)),
             alpha = 0.5,
             size = 1,
             show.legend = FALSE,
             shape = 16)+
  coord_cartesian(xlim = c(5000, 7000),
                  ylim = c(0.7, NA),
                  expand = F)+
  scale_colour_brewer(palette = "Spectral")+
  ggthemes::theme_few()+
  theme(axis.text = element_blank())+
  labs(x = "position index", y = "Y coordinate")
```

We prepare a plot of the smoothed data as well as the demonstration of the method relative to gaps.

```{r}
# prepare data to plot
# make list of data to plot
figure_median_smooth <- 
  mapply(function(df, col) {
    ggplot()+
      geom_point(data = df,
                 aes(x, y),
                 size = 0.5,
                 alpha = 0.5,
                 show.legend = FALSE,
                 col = col,
                 shape = 1)+
      geom_path(data = data,
                aes(x, y),
                col = "grey20")+
      coord_cartesian(expand = T,
                  ylim = c(0.6, 0.8)) +
      ggthemes::theme_map()
}, data_plot, c(pal[2], rep(
  RColorBrewer::brewer.pal(3, "Greens")[2],
  4)),
SIMPLIFY = F)

# make basic plot with rectangle
fig_bounds <-
  ggplot()+
  geom_point(data = data_errors,
            aes(x, y),
            size = 0.1, alpha = 0.2, 
            col = "grey")+
  geom_path(data = data,
            aes(x, y),
            size = 0.1)+
  geom_hline(yintercept = c(0.6, 0.8),
             col = "grey20",
             lty = 2, lwd = 0.2)+
  coord_equal(expand = F) +
  ggthemes::theme_map()

# append to first figure
figure_median_smooth[[1]] <-
  figure_median_smooth[[1]]+
  annotation_custom(grob = ggplotGrob(fig_bounds),
                    ymin = 0.6, ymax = 0.8,
                    xmin = 0.5, xmax = 0.7)

# wrap plots
fig_median_smooth <- wrap_plots(append(figure_median_smooth, 
                                       list(fig_smooth_1d)), 
                                       ncol = 3)+
  plot_annotation(tag_levels = "a",
                  tag_prefix = "(",
                  tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))

# save figure
ggsave(fig_median_smooth, filename = "figures/fig_median_smooth.png",
       width = 225, height = 225 / 1.77, units = "mm")
```

## Thinning Data by Aggregation

Evenly thinning data is a good idea if statistical methods require even sampling, or if the volume of data is too large for statistical packages to efficiently handle it. In `R`, both may be true at once.

Here, we demonstrate thinning by aggregation on data that has been median smoothed using a $K$ of 11.

```{r}
# choose the 11 point median smooth data
data_agg <- copy(list_of_smooths[[3]])

# get list of aggregated data
list_of_agg <- lapply(c(3, 10, 30, 120), function(z) {
  
  data_return <- atl_thin_data(data = data_agg,
                            interval = z,
                            method = "aggregate")
  
  data_return[, interval := z]
  
  return(data_return)
})
```

We shall prepare a figure showing the aggregation steps.

```{r}
# make figure of aggregation
figure_aggregate <- 
  mapply(FUN = function(df) {
  ggplot(data = df)+
      geom_point(data = list_of_smooths[[3]],
                 aes(x, y),
                 col = pal[3],
                 size = 0.3, alpha = 0.5)+
      geom_point(aes(x, y, size = SD),
                 colour = pal[4],
                 alpha = 1,
                 stroke = 1,
                 shape = 4, show.legend = F)+
      geom_path(aes(x, y),
                 colour = pal[4],
                 alpha = 1)+
      geom_path(data = data,
                lwd = 0.2,
                aes(x, y),
                col = "grey20")+
      coord_equal(ylim = c(0.6, 0.8))+
      ggthemes::theme_few()+
      theme(axis.title = element_blank(),
            axis.text = element_blank())
}, list_of_agg[2:3],
SIMPLIFY = FALSE)

figure_aggregate <- wrap_plots(figure_aggregate)+
    plot_annotation(tag_levels = "a",
                  tag_prefix = "(",
                  tag_suffix = ")") &
    theme(plot.tag = element_text(face = "bold"))

# save figure
ggsave(figure_aggregate, 
       filename = "figures/fig_aggregate_data.png",
       width = 185, height = 90, units = "mm")
```

## Aggregation Before Reducing Positioning Errors

Users may rightly wonder whether they can get away with aggregating their data, using a median aggregation function, and reduce data volumes, correct uneven sampling frequency, and reduce large-scale errors all in one go.

The answer to most questions in ecology is, "It depends". Median aggregation before correcting positioning errors can indeed be advantageous; for instance for faster visualisation while preserving the broad structure of a track.

However, there are drawbacks. The main one is that information is lost in the aggregation process, lending less power to steps such as smoothing applied after aggregation.
Further, aggregation (and indeed any kind of thinning) results in significantly different estimates of speed and distance from the real speed.

We show the effect of aggregating before any error correction here.

```{r}
# read data with errors
data_errors <- fread("data/data_errors.csv")
  
# aggregate before correction
list_of_agg <- lapply(c(3, 10, 30, 120), function(z) {
  data_return <- atl_thin_data(data = data_errors,
                            interval = z,
                            method = "aggregate")
  data_return[, interval := z]
  data_return[, speed := atl_get_speed(data_return)]
  return(data_return)
})

# get real speed
data[, speed := atl_get_speed(data)]
```

We prepare the figures.

```{r}
### plot figures
fig_agg_data <- 
  lapply(list_of_agg, function(df) {
    ggplot(df)+
      geom_path(aes(x,y), 
                # col = factor(interval), 
                # group = interval),
                lwd = 0.2)+
      geom_point(aes(x,y, 
                     size = SD,
                     shape = speed < quantile(data$speed, 0.95, na.rm = T),
                     col = speed < quantile(data$speed, 0.95, na.rm = T),
                     group = interval),
                 stroke = ifelse(df$speed < quantile(data$speed, 0.95, na.rm = T),
                                 1, 1),
                 show.legend = F)+
      geom_path(data = data,
                aes(x, y),
                lwd = 0.1)+
      scale_shape_manual(values = c(2, 4))+
      scale_colour_manual(values = pal[c(1, 2)])+
      coord_cartesian(ylim = c(0.6, NA))+
      ggthemes::theme_map()  
  })
```

```{r}
# now plot distribution of speed
data_agg <- rbindlist(list_of_agg)

# show boxplot of speed
fig_agg_speed <- ggplot(data_agg)+
  geom_jitter(aes(factor(interval), speed),
              col = "grey",
              shape = 4, alpha = 0.5)+
  geom_boxplot(aes(factor(interval), speed),
               fill = NA,
               show.legend = F, 
               width = 0.3, 
               outlier.size = 0.2)+
  geom_hline(yintercept =
               c(mean(data$speed, na.rm = T),
                 quantile(data$speed, na.rm = T,
                          probs = c(0.95))
               ),
             lty = c(1, 2))+
  scale_y_log10(label = scales::comma)+
  ggthemes::theme_few()+
  theme(axis.text.y = element_blank())+
  labs(x = "interval (s)",
       y = "speed")
```

```{r}
# make combined figure
fig_aggregate <- 
  wrap_plots(append(fig_agg_data[2:3], list(fig_agg_speed)),
           design = "AAABBBCC")+
  plot_annotation(tag_levels = "a",
                  tag_prefix = "(",
                  tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))

# save figure
ggsave(fig_aggregate,
       filename = "figures/fig_aggregate_errors.png",
       width = 170, height = 170 / 1.67, units = "mm")
```

![](figures/fig_aggregate_errors.png)

Thinning a movement track using median aggregation preserves track structure, but affects essential track metrics such as speed.
**(a, b)** Movement tracks with a canonical interval of 1s aggregated over intervals of **(a)** 10 and **(b)** 30 seconds, without removing large- or small-scale positioning errors. All symbols represent positions in the aggregated track, with the size of the symbol representing the standard deviation at each position.
Blue crosses represent positions with speed $\leq$ the 95^th percentile of canonical speeds, while red triangles represent positions with speed $\geq$ 95^th percentile of canonical speeds.
**(c)** Boxplot of instantaneous speeds after median aggregation of a 1s interval track over intervals of 3, 10, 30, and 120 seconds, but without the removal of positioning errors. The mean and 95^th percentile of speed in the canonical track are shown as solid and dashed lines, respectively. 
Aggregation without reducing positioning errors can result in speed estimates that are substantially different from the true speed.
